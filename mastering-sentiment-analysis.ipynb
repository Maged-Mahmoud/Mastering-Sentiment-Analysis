{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/magedmahmoud/mastering-sentiment-analysis?scriptVersionId=120075589\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Frame the problem and look at the big picture:\nIn this notebook we use various techniques and recent methods in text classification from rule based sentiment analysis to Machine learning based Sentiment Analysis.","metadata":{}},{"cell_type":"markdown","source":"**This Notebook will give step by step approach to NLP Sentiment Analysis from basic models to deep learning models** ","metadata":{}},{"cell_type":"markdown","source":"### What is Sentiment Analysis?\nSentiment Analysis is a NLP technique used to interpret the emotions, comments and reviews (Positive, Negative or Natural) behind text data.\n### Applications of Sentiment Analysis:\n* Social Media Analysis\n* Public Sentiment about products\n* Content Moderation\n* Stock Market Analysis\n","metadata":{}},{"cell_type":"markdown","source":"**Some Code here is imported from **SOHAIL notebook****","metadata":{}},{"cell_type":"markdown","source":"# Get The Data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-23T10:50:51.998493Z","iopub.execute_input":"2023-02-23T10:50:51.998948Z","iopub.status.idle":"2023-02-23T10:50:52.083528Z","shell.execute_reply.started":"2023-02-23T10:50:51.998852Z","shell.execute_reply":"2023-02-23T10:50:52.082743Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/financial-sentiment-analysis/data.csv\n/kaggle/input/fast-text-embeddings-without-subwords/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec\n/kaggle/input/fast-text-embeddings-without-subwords/crawl-300d-2M.vec/crawl-300d-2M.vec\n/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\n/kaggle/input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin.gz\n/kaggle/input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\n/kaggle/input/glove2word2vec/glove_w2v.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Import Files and Libraries","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install tensorflow_text\n!pip install transformers","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-02-23T10:50:57.039743Z","iopub.execute_input":"2023-02-23T10:50:57.040021Z","iopub.status.idle":"2023-02-23T10:52:04.660401Z","shell.execute_reply.started":"2023-02-23T10:50:57.039992Z","shell.execute_reply":"2023-02-23T10:52:04.659428Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# nltk imports\nfrom nltk.tokenize import word_tokenize  # tokenize the text == the text is splitted into words in list\nfrom nltk.corpus import stopwords  # this contain common stop words that has no effect in analysis\nfrom nltk.stem import WordNetLemmatizer  # Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item\n\n# sklearn imports\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # bags of words and TF IDF\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, make_scorer  # classification Metrics\nfrom sklearn.naive_bayes import MultinomialNB  # Multiclassification\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import svm\nfrom sklearn.model_selection import StratifiedKFold  # For stratified splitting (helpful in imbalanced data)\nfrom sklearn.preprocessing import LabelBinarizer  # for Categorical features\nfrom sklearn.model_selection import GridSearchCV  # for tuning parameters\nfrom sklearn.model_selection import train_test_split  # splitting dataset\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\nfrom sklearn import pipeline\nfrom sklearn import linear_model\n\n# gensim imports\nfrom gensim.models import KeyedVectors  # to save and load vectors\nimport string\nimport re\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport catboost as cbt\n\n# tensorflow and keras\nimport keras\nfrom keras import backend as K\nfrom tensorflow.keras.layers import Embedding\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras_preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import sequence, text\nfrom keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping , ReduceLROnPlateau\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\n# Hugging Face Transformers\nfrom transformers import (pipeline , BertTokenizer,\n                          TFBertForSequenceClassification,\n                          InputExample, InputFeatures , \n                         AutoTokenizer, TFAutoModelForSequenceClassification,\n                         TFRobertaModel, TFGPT2Model, RobertaTokenizer, GPT2Tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:52:04.662088Z","iopub.execute_input":"2023-02-23T10:52:04.662984Z","iopub.status.idle":"2023-02-23T10:52:14.045711Z","shell.execute_reply.started":"2023-02-23T10:52:04.662936Z","shell.execute_reply":"2023-02-23T10:52:14.044479Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/financial-sentiment-analysis/data.csv')\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:52:23.758457Z","iopub.execute_input":"2023-02-23T10:52:23.75888Z","iopub.status.idle":"2023-02-23T10:52:23.845906Z","shell.execute_reply.started":"2023-02-23T10:52:23.758846Z","shell.execute_reply":"2023-02-23T10:52:23.844912Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                            Sentence Sentiment\n0  The GeoSolutions technology will leverage Bene...  positive\n1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n2  For the last quarter of 2010 , Componenta 's n...  positive\n3  According to the Finnish-Russian Chamber of Co...   neutral\n4  The Swedish buyout firm has sold its remaining...   neutral","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The GeoSolutions technology will leverage Bene...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>For the last quarter of 2010 , Componenta 's n...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>According to the Finnish-Russian Chamber of Co...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The Swedish buyout firm has sold its remaining...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:52:24.217039Z","iopub.execute_input":"2023-02-23T10:52:24.217294Z","iopub.status.idle":"2023-02-23T10:52:24.224461Z","shell.execute_reply.started":"2023-02-23T10:52:24.217268Z","shell.execute_reply":"2023-02-23T10:52:24.223373Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(5842, 2)"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:52:24.412692Z","iopub.execute_input":"2023-02-23T10:52:24.412956Z","iopub.status.idle":"2023-02-23T10:52:24.44527Z","shell.execute_reply.started":"2023-02-23T10:52:24.412928Z","shell.execute_reply":"2023-02-23T10:52:24.443729Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5842 entries, 0 to 5841\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   Sentence   5842 non-null   object\n 1   Sentiment  5842 non-null   object\ndtypes: object(2)\nmemory usage: 91.4+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"unique_sentiments = df.Sentiment.unique()\nunique_sentiments","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:52:24.630857Z","iopub.execute_input":"2023-02-23T10:52:24.631131Z","iopub.status.idle":"2023-02-23T10:52:24.638198Z","shell.execute_reply.started":"2023-02-23T10:52:24.631102Z","shell.execute_reply":"2023-02-23T10:52:24.637646Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array(['positive', 'negative', 'neutral'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"df.Sentiment.value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:52:24.865156Z","iopub.execute_input":"2023-02-23T10:52:24.866098Z","iopub.status.idle":"2023-02-23T10:52:25.10063Z","shell.execute_reply.started":"2023-02-23T10:52:24.86603Z","shell.execute_reply":"2023-02-23T10:52:25.100122Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# mapping to the sentiment column \n\ndicto = {'positive': 1, 'neutral': 0 , 'negative': -1}\n\ndf.Sentiment = df.Sentiment.map(dicto)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:52:25.101747Z","iopub.execute_input":"2023-02-23T10:52:25.102065Z","iopub.status.idle":"2023-02-23T10:52:25.109856Z","shell.execute_reply.started":"2023-02-23T10:52:25.102039Z","shell.execute_reply":"2023-02-23T10:52:25.109064Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\ndef classical_model(df, bow=False, TFIDF=False, Ngram=False,\n                    model=linear_model.LogisticRegression(solver='liblinear')):\n    '''\n    Automates classical models to train and evaluate Sentiment Analysis Models.\n\n    Args:\n    df : pandas DataFrame\n        DataFrame with columns Sentence and Sentiment\n    bow : bool\n        Flag for using bag of words (binary, count, or frequency)\n    TFIDF : bool\n        Flag for using Tfidf vectorization\n    Ngram : tuple\n        Shape of Ngram range (e.g. (1,2) for bigrams)\n    model : scikit-learn estimator\n        Model to be used for training\n\n    Returns:\n    None\n    '''\n\n    df['kfold'] = -1 # Add a column to split data later\n    df = df.sample(frac=1).reset_index(drop=True) # Shuffle data and reset index\n\n    # Initiate kfold class from model_selection module\n    np.random.seed(0)\n    n_splits = 5\n    kf = StratifiedKFold(n_splits=n_splits)\n\n    for f, (train, val) in enumerate(kf.split(X=df, y=df.Sentiment)):\n        df.loc[val, 'kfold'] = f # Assign each row to its validation set number\n\n    if bow:\n        count_vec = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n    elif TFIDF:\n        count_vec = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n    elif Ngram:\n        count_vec = CountVectorizer(tokenizer=word_tokenize, token_pattern=None, ngram_range=Ngram)\n\n    for fold_ in range(n_splits):\n        train_df = df[df.kfold != fold_].reset_index(drop=True)\n        test_df = df[df.kfold == fold_].reset_index(drop=True)\n        count_vec.fit(train_df.Sentence)\n        xtrain = count_vec.transform(train_df.Sentence)\n        xtest = count_vec.transform(test_df.Sentence)\n        model.fit(xtrain, train_df.Sentiment)\n        preds = model.predict(xtest)\n        accuracy_precision = precision_score(test_df.Sentiment, preds, average='macro')\n        accuracy_recall = recall_score(test_df.Sentiment, preds, average='macro')\n        print('precision score:', accuracy_precision)\n        print('recall score:', accuracy_recall)\n        print(\"========================================================\")\n\n    print(classification_report(test_df.Sentiment, preds))","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:52:25.309893Z","iopub.execute_input":"2023-02-23T10:52:25.310167Z","iopub.status.idle":"2023-02-23T10:52:25.323005Z","shell.execute_reply.started":"2023-02-23T10:52:25.310137Z","shell.execute_reply":"2023-02-23T10:52:25.322039Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Model-1 \n### Logistic + BOW","metadata":{}},{"cell_type":"code","source":"#Baseline model let's start with a logistic regression model since it is the fastest for high dimensional sparse data\n\nclassical_model(df, bow =True,model=linear_model.LogisticRegression(solver = 'liblinear'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that labels **1,0 have more than 70 percent** score but label **-1 score is pretty low**, hence we can conclude that we need to go on modelling untill these scores get better.","metadata":{}},{"cell_type":"markdown","source":"# Model 2 NaiveBayes + BOW","metadata":{}},{"cell_type":"code","source":"# Lets try with NaiveBayes model \n\nclassical_model(df,bow = True,model = MultinomialNB()) # multiclassification","metadata":{"execution":{"iopub.status.busy":"2023-02-22T03:56:48.570741Z","iopub.execute_input":"2023-02-22T03:56:48.571026Z","iopub.status.idle":"2023-02-22T03:56:57.397393Z","shell.execute_reply.started":"2023-02-22T03:56:48.57099Z","shell.execute_reply":"2023-02-22T03:56:57.396534Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"precision score: 0.6228029247397492\nrecall score: 0.5869766066681366\n========================================================\nprecision score: 0.6584857466391166\nrecall score: 0.6133894079299903\n========================================================\nprecision score: 0.610298418490932\nrecall score: 0.5702695674350329\n========================================================\nprecision score: 0.6173100148643628\nrecall score: 0.5800834231958386\n========================================================\nprecision score: 0.6334566048227578\nrecall score: 0.5991385248090798\n========================================================\n              precision    recall  f1-score   support\n\n          -1       0.43      0.29      0.35       172\n           0       0.74      0.84      0.78       626\n           1       0.73      0.66      0.70       370\n\n    accuracy                           0.70      1168\n   macro avg       0.63      0.60      0.61      1168\nweighted avg       0.69      0.70      0.69      1168\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It is evident that the for **label -1 the metrics values little improved** so this model will be the new baseline model for this problem.","metadata":{}},{"cell_type":"markdown","source":"# Model-3 Naive Bayes with TFIDF","metadata":{}},{"cell_type":"code","source":"# Now lets try with TF-IDF vectorizer instead of bag of words to MultinomialNB().\n\nclassical_model(df, model= MultinomialNB(),TFIDF=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T03:56:57.398615Z","iopub.execute_input":"2023-02-22T03:56:57.399491Z","iopub.status.idle":"2023-02-22T03:57:06.137042Z","shell.execute_reply.started":"2023-02-22T03:56:57.399449Z","shell.execute_reply":"2023-02-22T03:57:06.136148Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"precision score: 0.8032709303221403\nrecall score: 0.4716788874026719\n========================================================\nprecision score: 0.7965176893969463\nrecall score: 0.46977391388054385\n========================================================\nprecision score: 0.7755521472392638\nrecall score: 0.44167046532378285\n========================================================\nprecision score: 0.6706332757628094\nrecall score: 0.4582149725325296\n========================================================\nprecision score: 0.6951144094001237\nrecall score: 0.45724589661360593\n========================================================\n              precision    recall  f1-score   support\n\n          -1       0.75      0.02      0.03       172\n           0       0.64      0.98      0.77       626\n           1       0.70      0.37      0.48       370\n\n    accuracy                           0.65      1168\n   macro avg       0.70      0.46      0.43      1168\nweighted avg       0.67      0.65      0.57      1168\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"also, the **label -1 metrics recall score is unsatisfied**  ","metadata":{}},{"cell_type":"markdown","source":"# Model-4 Naive Bayes with Ngrams","metadata":{}},{"cell_type":"code","source":"# Now to the baseline bag of word Naivebayes model lets apply Ngrams  and compare the results.\n\n# Lets try with NaiveBayes model \n\nclassical_model(df, model = MultinomialNB(), Ngram=(1,2))","metadata":{"execution":{"iopub.status.busy":"2023-02-22T03:57:06.139416Z","iopub.execute_input":"2023-02-22T03:57:06.139908Z","iopub.status.idle":"2023-02-22T03:57:17.298584Z","shell.execute_reply.started":"2023-02-22T03:57:06.139865Z","shell.execute_reply":"2023-02-22T03:57:17.297073Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"precision score: 0.571222426085165\nrecall score: 0.5456817640637264\n========================================================\nprecision score: 0.5705919969119083\nrecall score: 0.5462114750418011\n========================================================\nprecision score: 0.5870069211261302\nrecall score: 0.5530476943992064\n========================================================\nprecision score: 0.5673112087008567\nrecall score: 0.5451441113451664\n========================================================\nprecision score: 0.5778658016420254\nrecall score: 0.5594099263093988\n========================================================\n              precision    recall  f1-score   support\n\n          -1       0.27      0.16      0.20       172\n           0       0.72      0.84      0.78       626\n           1       0.74      0.68      0.71       370\n\n    accuracy                           0.69      1168\n   macro avg       0.58      0.56      0.56      1168\nweighted avg       0.66      0.69      0.67      1168\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"So the **ngrams has even decreased the metrics** value.Hence it is evident that the baseline model is still the best one till now","metadata":{}},{"cell_type":"markdown","source":"# Text processing","metadata":{}},{"cell_type":"markdown","source":"I will **not remove all stopwords** in this because it might change the context of the sentence.\n\ne.g **\"He is not a good person\"** will be changed into **\" 'He' , 'good', 'person'\"** which is the complete opposite of the sentence.","metadata":{}},{"cell_type":"code","source":"# Now lets do some cleaning on the text data and apply it to baseline model and compare the accuracies.\nstop_words = [i for i in stopwords.words('english') if \"n't\" not in i and i not in ('not','no')]\n\ndef process_text(text):\n    \n    text = word_tokenize(text) # tokenize words in text\n    text = [re.sub('[^A-Za-z]+', '', word) for word in text] # this line substitutes any white space before the word by removing the space\n    text = [word.lower() for word in text if word.isalpha()] # lower each word in text\n    text = [word for word in text if word not in stop_words]\n    text = [WordNetLemmatizer().lemmatize(word) for word in text] # lemmatization of words, so when see persons an person, both are dealt as one word person\n    text = ' '.join(text) # join words into text again\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = 'He is not a good person'\ntoken_text = word_tokenize(text)\n[ word for word in token_text if word not in stop_words]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Sentence = df.Sentence.apply(process_text) # this line applies process_text function to Sentence in dataset\ndf.Sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 5 cleaned text Naive Bayes + BOW  ","metadata":{}},{"cell_type":"code","source":"#Now lets try this on our baseline MultinomialNB bagofwords model\n\nclassical_model(df, model = MultinomialNB(),bow = True)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T03:57:20.947451Z","iopub.execute_input":"2023-02-22T03:57:20.947789Z","iopub.status.idle":"2023-02-22T03:57:27.033678Z","shell.execute_reply.started":"2023-02-22T03:57:20.947753Z","shell.execute_reply":"2023-02-22T03:57:27.032841Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"precision score: 0.6421374863757188\nrecall score: 0.6138530969600732\n========================================================\nprecision score: 0.6007498465847156\nrecall score: 0.5834384076974782\n========================================================\nprecision score: 0.6196447230929989\nrecall score: 0.5952158340077204\n========================================================\nprecision score: 0.6034499172426201\nrecall score: 0.5944203583924218\n========================================================\nprecision score: 0.610425472494438\nrecall score: 0.5994605572211634\n========================================================\n              precision    recall  f1-score   support\n\n          -1       0.36      0.33      0.34       172\n           0       0.75      0.80      0.77       626\n           1       0.72      0.68      0.70       370\n\n    accuracy                           0.69      1168\n   macro avg       0.61      0.60      0.60      1168\nweighted avg       0.68      0.69      0.69      1168\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"after some text cleaning **we beat our baseline model by small margin** hence the above model will be our new baseline model.","metadata":{}},{"cell_type":"markdown","source":"Now lets go into **word embeddings** \n\nIn the above models each word token is converted into integer tokens by (BOW and TFIDF) now lets convert these **integer tokens into vectors.**","metadata":{}},{"cell_type":"markdown","source":"# Model 6 Fastext vector with Naive Bayes Baseline ","metadata":{}},{"cell_type":"code","source":"# In this model we will use fastText vectors and also convert each word vector in sentence vector.\n# The code is taken from https://fasttext.cc/docs/en/english-vectors.html, this code splits each vector by \n# space and return for more info go through the above link.\n\ndef sentence_to_vec(sentence, embedding_dict, tokenizer):\n    # This function converts a sentence to a vector of word vectors\n    words = tokenizer(sentence)\n    embedding_list = []\n    for word in words:\n        if word in embedding_dict:\n            embedding_list.append(embedding_dict[word])\n    if len(embedding_list) == 0:\n        # if no vectors are found, return zeros\n        return np.zeros(300)\n    embedding_list = np.array(embedding_list)\n    vector = embedding_list.sum(axis=0) / len(embedding_list)\n    return vector\n\n\ndf = df.sample(frac=1).reset_index(drop=True)\ny = df['Sentiment'].values\ndf.drop('kfold', axis=1, inplace=True)\nkf = StratifiedKFold(n_splits=5)\n\n# Load embeddings into memory\nprint(\"Loading embeddings\")\nembeddings = KeyedVectors.load_word2vec_format('/kaggle/input/fast-text-embeddings-without-subwords/crawl-300d-2M.vec/crawl-300d-2M.vec')\n\n# Create sentence embeddings\nprint(\"Creating sentence vectors\")\nvectors = []\nfor sentence in df['Sentence'].values:\n    vectors.append(sentence_to_vec(sentence=sentence, embedding_dict=embeddings, tokenizer=word_tokenize))\nvectors = np.array(vectors)\n\nfor fold_, (train_, valid_) in enumerate(kf.split(X=df, y=y)):\n    print(\"Fold: \", fold_)\n    x_train = vectors[train_, :]\n    y_train = y[train_]\n    x_test = vectors[valid_, :]\n    y_test = y[valid_]\n    scaler = MinMaxScaler()\n    x_train_scaled = scaler.fit_transform(x_train)\n    x_test_scaled = scaler.transform(x_test)\n    model = MultinomialNB()\n    model.fit(x_train_scaled, y_train)\n    y_pred = model.predict(x_test_scaled)\n    pres_score = precision_score(y_test, y_pred, average='macro')\n    rec_score = recall_score(y_test, y_pred, average='macro')\n    print('Precision and recall scores:', pres_score, rec_score)\n    print(\"======================================================\")\n\nprint(classification_report(y_test, y_pred, labels=[1, -1, 0]))","metadata":{"execution":{"iopub.status.busy":"2023-02-22T03:57:27.035194Z","iopub.execute_input":"2023-02-22T03:57:27.035482Z","iopub.status.idle":"2023-02-22T04:03:01.881796Z","shell.execute_reply.started":"2023-02-22T03:57:27.035443Z","shell.execute_reply":"2023-02-22T04:03:01.879991Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Loading embeddings\nCreating sentence vectors\nFold:  0\nPrecision and recall scores: 0.40322424944492713 0.34358252312921067\n======================================================\nFold:  1\nPrecision and recall scores: 0.3396240744256693 0.3394891623537111\n======================================================\nFold:  2\nPrecision and recall scores: 0.3666049923718635 0.3441844400310854\n======================================================\nFold:  3\nPrecision and recall scores: 0.3590900214801797 0.3440203782056817\n======================================================\nFold:  4\nPrecision and recall scores: 0.3642276422764228 0.3416457991537864\n======================================================\n              precision    recall  f1-score   support\n\n           1       0.55      0.03      0.06       370\n          -1       0.00      0.00      0.00       172\n           0       0.54      1.00      0.70       626\n\n    accuracy                           0.54      1168\n   macro avg       0.36      0.34      0.25      1168\nweighted avg       0.47      0.54      0.39      1168\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Whoa! It is our worst performing model so far! What should we do now? Should we ask muppets to save us? No! we will keep on trying with other models and methods.Thats what data scientists do!**","metadata":{}},{"cell_type":"markdown","source":"# Model 7 - Glove vectors with Baseline model","metadata":{}},{"cell_type":"code","source":"\n\nclass Word2VecVectorizer:\n    \n    \n    def __init__(self, word_vectors):\n        print(\"Loading in word vectors...\")\n        self.word_vectors = word_vectors\n        print(\"Finished loading in word vectors\")\n\n    def fit(self, data):\n        pass\n\n    def transform(self, data):\n        # Determine the dimensionality of vectors\n        v = self.word_vectors.get_vector('king')\n        self.D = v.shape[0]\n\n        X = np.zeros((len(data), self.D))\n        n = 0\n        empty_count = 0\n\n        for sentence in data:\n            tokens = sentence.split()\n            vecs = []\n            m = 0\n\n            for word in tokens:\n                try:\n                    vec = self.word_vectors.get_vector(word)\n                    vecs.append(vec)\n                    m += 1\n                except KeyError:\n                    pass\n\n            if len(vecs) > 0:\n                vecs = np.array(vecs)\n                X[n] = vecs.mean(axis=0)\n            else:\n                empty_count += 1\n\n            n += 1\n\n        print(\"Number of samples with no words found: %s / %s\" % (empty_count, len(data)))\n        return X\n\n    def fit_transform(self, data):\n        self.fit(data)\n        return self.transform(data)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:03:01.884393Z","iopub.execute_input":"2023-02-22T04:03:01.884675Z","iopub.status.idle":"2023-02-22T04:03:01.903584Z","shell.execute_reply.started":"2023-02-22T04:03:01.88462Z","shell.execute_reply":"2023-02-22T04:03:01.901692Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"word2vec_output_file = '/kaggle/input/glove2word2vec/glove_w2v.txt'\nword_vectors = KeyedVectors.load_word2vec_format(word2vec_output_file, binary = False)\nXtrain,Xtest,ytrain,ytest = train_test_split(df.Sentence, df.Sentiment, test_size = 0.2, \n                                             random_state = 42,stratify =df.Sentiment )","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:03:01.904744Z","iopub.execute_input":"2023-02-22T04:03:01.905049Z","iopub.status.idle":"2023-02-22T04:03:48.860235Z","shell.execute_reply.started":"2023-02-22T04:03:01.905015Z","shell.execute_reply":"2023-02-22T04:03:48.85944Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#we create a Vectorizer object that will help us to transform our reviews to vectors, a numerical representation. \n#Then we can use those vectors to feed our classifier.\n\nvectorizer = Word2VecVectorizer(word_vectors)\n\nX_train = vectorizer.fit_transform(Xtrain)\ny_train = ytrain\n\nX_test = vectorizer.transform(Xtest)\ny_test = ytest","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:03:48.861605Z","iopub.execute_input":"2023-02-22T04:03:48.861859Z","iopub.status.idle":"2023-02-22T04:03:49.063352Z","shell.execute_reply.started":"2023-02-22T04:03:48.861824Z","shell.execute_reply":"2023-02-22T04:03:49.062603Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Loading in word vectors...\nFinished loading in word vectors\nNumber of samples with no words found: 0 / 4673\nNumber of samples with no words found: 0 / 1169\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the parameters to tune\nparameters = {\n    'C': [0.5, 1.0, 3],\n    'gamma': [1, 'auto', 'scale'],\n    'kernel': ['rbf','linear']\n}\n\nmodel = GridSearchCV(SVC(), parameters, cv=5, n_jobs=-1,verbose=1)\n\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test,y_pred))\nprint(model.best_params_)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:03:49.064619Z","iopub.execute_input":"2023-02-22T04:03:49.064857Z","iopub.status.idle":"2023-02-22T04:07:55.688266Z","shell.execute_reply.started":"2023-02-22T04:03:49.064825Z","shell.execute_reply":"2023-02-22T04:07:55.687364Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 18 candidates, totalling 90 fits\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.1min\n[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  4.0min finished\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n          -1       0.60      0.20      0.30       172\n           0       0.72      0.92      0.80       626\n           1       0.75      0.63      0.69       371\n\n    accuracy                           0.72      1169\n   macro avg       0.69      0.58      0.60      1169\nweighted avg       0.71      0.72      0.69      1169\n\n{'C': 1.0, 'gamma': 'scale', 'kernel': 'rbf'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Model 7 is our best performing model,then model 5, lets do some hyperparameter tuning before we move on to deep learning models**","metadata":{}},{"cell_type":"markdown","source":"# Model # Hyperparameter tuning best models","metadata":{}},{"cell_type":"code","source":"# model 5 Tuning\ndef scoring(y_train,y_pred):\n    return f1_score(y_train,y_pred,average='macro')\n\ncount_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern = None)\n\nX_train = count_vec.fit_transform(df.Sentence)\n\nparameters = {'alpha': [0.001,0.01,0.1,0.2,0.3,0.5,0.7,1,1.5,1.6,1.8,10,100]}\n\nmodel = MultinomialNB()\n\ngrid_search = GridSearchCV(model , parameters, cv=5, scoring = make_scorer(scoring), n_jobs = -1, verbose= 1)\n\ngrid_result = grid_search.fit(X_train, df.Sentiment)\n\nprint('Best params: ', grid_result.best_params_)\nprint('Best score: ', grid_result.best_score_)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:07:55.689847Z","iopub.execute_input":"2023-02-22T04:07:55.690298Z","iopub.status.idle":"2023-02-22T04:07:56.972607Z","shell.execute_reply.started":"2023-02-22T04:07:55.690253Z","shell.execute_reply":"2023-02-22T04:07:56.971837Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 13 candidates, totalling 65 fits\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","output_type":"stream"},{"name":"stdout","text":"Best params:  {'alpha': 0.7}\nBest score:  0.6076783924643652\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Done  65 out of  65 | elapsed:    0.4s finished\n","output_type":"stream"}]},{"cell_type":"code","source":"# Lets also check svm hyperparameters using pipelines\n\ndf = df.sample(frac=1,random_state=42).reset_index(drop=True)\n\n\n\ncount_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern = None)\n\nX_train = count_vec.fit_transform(df.Sentence)\n\n# defining parameter range\nparam_grid = {'C': [0.1, 1, 10, 100],\n              'gamma': [1, 0.1, 0.01, 0.001],\n              'kernel': ['rbf','linear']}\n \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 1, cv=5, scoring = make_scorer(scoring), n_jobs = -1)\n \n# fitting the model for grid search\ngrid.fit(X_train, df.Sentiment)\n\nprint('Best params: ', grid.best_params_)\nprint('Best score: ', grid.best_score_)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:08:27.940926Z","iopub.execute_input":"2023-02-22T04:08:27.941492Z","iopub.status.idle":"2023-02-22T04:15:37.682808Z","shell.execute_reply.started":"2023-02-22T04:08:27.941453Z","shell.execute_reply":"2023-02-22T04:15:37.682114Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 32 candidates, totalling 160 fits\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  1.7min\n[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:  7.1min finished\n","output_type":"stream"},{"name":"stdout","text":"Best params:  {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\nBest score:  0.5597634166585358\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**The model has performed worser than the previous above model, we can also create different pipelines of different machine learning models and compare them with both BOW and tfidf models**\n\n**More models including deep learning models are yet to come**\n","metadata":{}},{"cell_type":"markdown","source":"# Deep Learning Models:","metadata":{}},{"cell_type":"markdown","source":"# Model- 8 LSTM using glove word2vec","metadata":{}},{"cell_type":"code","source":"# loading pretrained google news word2vec embedding 300D\nword2vec_pretrained = KeyedVectors.load_word2vec_format(\"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\",binary=True)\nword2vec_pretrained_dict = dict(zip(word2vec_pretrained.key_to_index.keys(),\n                                    word2vec_pretrained.vectors))","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:15:47.779808Z","iopub.execute_input":"2023-02-22T04:15:47.780079Z","iopub.status.idle":"2023-02-22T04:16:56.933288Z","shell.execute_reply.started":"2023-02-22T04:15:47.780042Z","shell.execute_reply":"2023-02-22T04:16:56.932485Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"list(word2vec_pretrained_dict.values())[0].shape","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:16:56.934827Z","iopub.execute_input":"2023-02-22T04:16:56.935092Z","iopub.status.idle":"2023-02-22T04:16:57.039456Z","shell.execute_reply.started":"2023-02-22T04:16:56.935053Z","shell.execute_reply":"2023-02-22T04:16:57.038604Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"(300,)"},"metadata":{}}]},{"cell_type":"code","source":"\ndf['Sentence'] = df['Sentence'].apply(process_text)\n\nX_train,X_test,y_train,y_test = train_test_split(df.Sentence, df.Sentiment, test_size = 0.2,\n                                                 random_state = 42, stratify= df.Sentiment, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T09:58:52.659962Z","iopub.execute_input":"2023-02-22T09:58:52.660727Z","iopub.status.idle":"2023-02-22T09:58:53.85237Z","shell.execute_reply.started":"2023-02-22T09:58:52.660689Z","shell.execute_reply":"2023-02-22T09:58:53.851645Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"y_train_enc = np_utils.to_categorical(y_train, 3)\ny_test_enc = np_utils.to_categorical(y_test, 3)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T09:58:56.154533Z","iopub.execute_input":"2023-02-22T09:58:56.154807Z","iopub.status.idle":"2023-02-22T09:58:56.159148Z","shell.execute_reply.started":"2023-02-22T09:58:56.154776Z","shell.execute_reply":"2023-02-22T09:58:56.158471Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"token = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n\ntoken.fit_on_texts(list(X_train) + list(X_test)) # fits tokens on texts\nxtrain_seq = token.texts_to_sequences(X_train) # text to sequences converts the sentence words to number sequences\nxtest_seq = token.texts_to_sequences(X_test)\n\n#zero pad sequences\nxtrain_pad = pad_sequences(xtrain_seq,padding='post') # zero padding all sentences to have the same shape as the largest one\nxtest_pad = pad_sequences(xtest_seq,padding='post')\n\nword_index = token.word_index # returns the word index that have been tokenized\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T09:59:52.598635Z","iopub.execute_input":"2023-02-22T09:59:52.598932Z","iopub.status.idle":"2023-02-22T09:59:52.79194Z","shell.execute_reply.started":"2023-02-22T09:59:52.598884Z","shell.execute_reply":"2023-02-22T09:59:52.791235Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#create embedding matrix for words that we have in dataset\n\nembedding_matrix = np.zeros((len(word_index)+1, 300))\nfor word,i in word_index.items():\n    embedding_vector = word2vec_pretrained_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        ","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:17:04.461792Z","iopub.execute_input":"2023-02-22T04:17:04.46209Z","iopub.status.idle":"2023-02-22T04:17:04.496723Z","shell.execute_reply.started":"2023-02-22T04:17:04.462047Z","shell.execute_reply":"2023-02-22T04:17:04.495848Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Build Custom Metrics (F1-Score)\n\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m_():\n    def f1_m(y_true, y_pred):\n        precision = precision_m(y_true, y_pred)\n        recall = recall_m(y_true, y_pred)\n        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n    return f1_m\n","metadata":{"execution":{"iopub.status.busy":"2023-02-23T04:10:17.065913Z","iopub.execute_input":"2023-02-23T04:10:17.066215Z","iopub.status.idle":"2023-02-23T04:10:17.073982Z","shell.execute_reply.started":"2023-02-23T04:10:17.066183Z","shell.execute_reply":"2023-02-23T04:10:17.072924Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# A simple LSTM with two dense layers\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index)+1,300,weights=[embedding_matrix], trainable = False))\n\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3))\n\nmodel.add(Dense(1024 , activation = 'relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation = 'relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = [f1_m_()])","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:17:11.54522Z","iopub.execute_input":"2023-02-22T04:17:11.545694Z","iopub.status.idle":"2023-02-22T04:17:11.799461Z","shell.execute_reply.started":"2023-02-22T04:17:11.545656Z","shell.execute_reply":"2023-02-22T04:17:11.798698Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model.fit(xtrain_pad, y=y_train_enc, batch_size = 512, epochs =10, verbose=1, validation_data = (xtest_pad, y_test_enc))","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:17:26.602915Z","iopub.execute_input":"2023-02-22T04:17:26.60321Z","iopub.status.idle":"2023-02-22T04:20:52.281772Z","shell.execute_reply.started":"2023-02-22T04:17:26.603178Z","shell.execute_reply":"2023-02-22T04:20:52.280979Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Epoch 1/10\n10/10 [==============================] - 22s 2s/step - loss: 1.0296 - f1_m: 0.1921 - val_loss: 0.9718 - val_f1_m: 0.5062\nEpoch 2/10\n10/10 [==============================] - 16s 2s/step - loss: 0.9546 - f1_m: 0.4834 - val_loss: 0.9960 - val_f1_m: 0.4698\nEpoch 3/10\n10/10 [==============================] - 18s 2s/step - loss: 0.9157 - f1_m: 0.4882 - val_loss: 0.8874 - val_f1_m: 0.5226\nEpoch 4/10\n10/10 [==============================] - 17s 2s/step - loss: 0.8864 - f1_m: 0.5030 - val_loss: 0.8715 - val_f1_m: 0.5020\nEpoch 5/10\n10/10 [==============================] - 18s 2s/step - loss: 0.8717 - f1_m: 0.5068 - val_loss: 0.8566 - val_f1_m: 0.4890\nEpoch 6/10\n10/10 [==============================] - 17s 2s/step - loss: 0.8650 - f1_m: 0.5289 - val_loss: 0.8440 - val_f1_m: 0.5366\nEpoch 7/10\n10/10 [==============================] - 16s 2s/step - loss: 0.8453 - f1_m: 0.5315 - val_loss: 0.8178 - val_f1_m: 0.5490\nEpoch 8/10\n10/10 [==============================] - 16s 2s/step - loss: 0.8304 - f1_m: 0.5572 - val_loss: 0.8095 - val_f1_m: 0.5969\nEpoch 9/10\n10/10 [==============================] - 16s 2s/step - loss: 0.8363 - f1_m: 0.5829 - val_loss: 0.8113 - val_f1_m: 0.5934\nEpoch 10/10\n10/10 [==============================] - 16s 2s/step - loss: 0.8383 - f1_m: 0.5651 - val_loss: 0.8253 - val_f1_m: 0.5719\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f5eb8526f50>"},"metadata":{}}]},{"cell_type":"markdown","source":"**The problem with training neural networks is in the choice of the number of training epochs to use.Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset.**\n\n","metadata":{}},{"cell_type":"markdown","source":"# Model 9 LSTM word2vec + early stopping","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics = [f1_m_()])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\nhistory = model.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:20:52.28361Z","iopub.execute_input":"2023-02-22T04:20:52.283895Z","iopub.status.idle":"2023-02-22T04:35:38.429627Z","shell.execute_reply.started":"2023-02-22T04:20:52.283858Z","shell.execute_reply":"2023-02-22T04:35:38.428801Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Epoch 1/100\n10/10 [==============================] - 20s 2s/step - loss: 1.0554 - f1_m: 0.1311 - val_loss: 1.0128 - val_f1_m: 0.0000e+00\nEpoch 2/100\n10/10 [==============================] - 17s 2s/step - loss: 0.9957 - f1_m: 0.2546 - val_loss: 0.9718 - val_f1_m: 0.5386\nEpoch 3/100\n10/10 [==============================] - 16s 2s/step - loss: 0.9576 - f1_m: 0.4893 - val_loss: 0.9319 - val_f1_m: 0.4170\nEpoch 4/100\n10/10 [==============================] - 17s 2s/step - loss: 0.9157 - f1_m: 0.4839 - val_loss: 0.8894 - val_f1_m: 0.4951\nEpoch 5/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8874 - f1_m: 0.4954 - val_loss: 0.8608 - val_f1_m: 0.5292\nEpoch 6/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8728 - f1_m: 0.5244 - val_loss: 0.8872 - val_f1_m: 0.5017\nEpoch 7/100\n10/10 [==============================] - 15s 2s/step - loss: 0.8753 - f1_m: 0.5253 - val_loss: 0.8567 - val_f1_m: 0.5458\nEpoch 8/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8583 - f1_m: 0.5395 - val_loss: 0.8639 - val_f1_m: 0.5082\nEpoch 9/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8530 - f1_m: 0.5419 - val_loss: 0.8427 - val_f1_m: 0.5280\nEpoch 10/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8511 - f1_m: 0.5210 - val_loss: 0.8263 - val_f1_m: 0.5566\nEpoch 11/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8444 - f1_m: 0.5574 - val_loss: 0.8223 - val_f1_m: 0.5978\nEpoch 12/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8241 - f1_m: 0.5833 - val_loss: 0.7973 - val_f1_m: 0.6216\nEpoch 13/100\n10/10 [==============================] - 15s 2s/step - loss: 0.8402 - f1_m: 0.5772 - val_loss: 0.8214 - val_f1_m: 0.5957\nEpoch 14/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8304 - f1_m: 0.5765 - val_loss: 0.7978 - val_f1_m: 0.6115\nEpoch 15/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8280 - f1_m: 0.5718 - val_loss: 0.8397 - val_f1_m: 0.5714\nEpoch 16/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8399 - f1_m: 0.5623 - val_loss: 0.8078 - val_f1_m: 0.6133\nEpoch 17/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8098 - f1_m: 0.5986 - val_loss: 0.8108 - val_f1_m: 0.5906\nEpoch 18/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8166 - f1_m: 0.6002 - val_loss: 0.7965 - val_f1_m: 0.6095\nEpoch 19/100\n10/10 [==============================] - 15s 2s/step - loss: 0.8086 - f1_m: 0.6065 - val_loss: 0.8101 - val_f1_m: 0.6188\nEpoch 20/100\n10/10 [==============================] - 16s 2s/step - loss: 0.7881 - f1_m: 0.6181 - val_loss: 0.7945 - val_f1_m: 0.6132\nEpoch 21/100\n10/10 [==============================] - 16s 2s/step - loss: 0.7889 - f1_m: 0.6155 - val_loss: 0.7589 - val_f1_m: 0.6325\nEpoch 22/100\n10/10 [==============================] - 16s 2s/step - loss: 0.7681 - f1_m: 0.6199 - val_loss: 0.7523 - val_f1_m: 0.6256\nEpoch 23/100\n10/10 [==============================] - 15s 2s/step - loss: 0.7424 - f1_m: 0.6134 - val_loss: 0.7384 - val_f1_m: 0.6177\nEpoch 24/100\n10/10 [==============================] - 16s 2s/step - loss: 0.7732 - f1_m: 0.6052 - val_loss: 0.7481 - val_f1_m: 0.6632\nEpoch 25/100\n10/10 [==============================] - 16s 2s/step - loss: 0.7570 - f1_m: 0.6501 - val_loss: 0.7346 - val_f1_m: 0.6291\nEpoch 26/100\n10/10 [==============================] - 16s 2s/step - loss: 0.7112 - f1_m: 0.6480 - val_loss: 0.6792 - val_f1_m: 0.6902\nEpoch 27/100\n10/10 [==============================] - 15s 1s/step - loss: 0.7041 - f1_m: 0.6679 - val_loss: 0.6547 - val_f1_m: 0.7054\nEpoch 28/100\n10/10 [==============================] - 16s 2s/step - loss: 0.7029 - f1_m: 0.6727 - val_loss: 0.6719 - val_f1_m: 0.6786\nEpoch 29/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6926 - f1_m: 0.6625 - val_loss: 0.6322 - val_f1_m: 0.7121\nEpoch 30/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6615 - f1_m: 0.7062 - val_loss: 0.6506 - val_f1_m: 0.7105\nEpoch 31/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6794 - f1_m: 0.6891 - val_loss: 0.6672 - val_f1_m: 0.6836\nEpoch 32/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6555 - f1_m: 0.6801 - val_loss: 0.6399 - val_f1_m: 0.6849\nEpoch 33/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6905 - f1_m: 0.6797 - val_loss: 0.6781 - val_f1_m: 0.6994\nEpoch 34/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6744 - f1_m: 0.6841 - val_loss: 0.6204 - val_f1_m: 0.7088\nEpoch 35/100\n10/10 [==============================] - 15s 2s/step - loss: 0.6451 - f1_m: 0.7033 - val_loss: 0.6458 - val_f1_m: 0.6913\nEpoch 36/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6369 - f1_m: 0.7002 - val_loss: 0.6330 - val_f1_m: 0.7017\nEpoch 37/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6136 - f1_m: 0.7181 - val_loss: 0.6390 - val_f1_m: 0.7072\nEpoch 38/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5991 - f1_m: 0.7299 - val_loss: 0.6068 - val_f1_m: 0.7266\nEpoch 39/100\n10/10 [==============================] - 15s 1s/step - loss: 0.6070 - f1_m: 0.7178 - val_loss: 0.6132 - val_f1_m: 0.7212\nEpoch 40/100\n10/10 [==============================] - 17s 2s/step - loss: 0.6028 - f1_m: 0.7221 - val_loss: 0.6207 - val_f1_m: 0.7229\nEpoch 41/100\n10/10 [==============================] - 15s 1s/step - loss: 0.5910 - f1_m: 0.7296 - val_loss: 0.6272 - val_f1_m: 0.7180\nEpoch 42/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5839 - f1_m: 0.7331 - val_loss: 0.6249 - val_f1_m: 0.7106\nEpoch 43/100\n10/10 [==============================] - 15s 2s/step - loss: 0.5551 - f1_m: 0.7448 - val_loss: 0.6296 - val_f1_m: 0.7025\nEpoch 44/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5568 - f1_m: 0.7436 - val_loss: 0.6045 - val_f1_m: 0.7101\nEpoch 45/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5432 - f1_m: 0.7495 - val_loss: 0.6013 - val_f1_m: 0.7301\nEpoch 46/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5352 - f1_m: 0.7625 - val_loss: 0.5929 - val_f1_m: 0.7184\nEpoch 47/100\n10/10 [==============================] - 15s 2s/step - loss: 0.5443 - f1_m: 0.7553 - val_loss: 0.6399 - val_f1_m: 0.7211\nEpoch 48/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5379 - f1_m: 0.7511 - val_loss: 0.6230 - val_f1_m: 0.7190\nEpoch 49/100\n10/10 [==============================] - 15s 2s/step - loss: 0.5133 - f1_m: 0.7642 - val_loss: 0.5986 - val_f1_m: 0.7179\nEpoch 50/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5363 - f1_m: 0.7531 - val_loss: 0.6075 - val_f1_m: 0.7197\nEpoch 51/100\n10/10 [==============================] - 15s 1s/step - loss: 0.5207 - f1_m: 0.7621 - val_loss: 0.6235 - val_f1_m: 0.7247\nEpoch 52/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5210 - f1_m: 0.7543 - val_loss: 0.6234 - val_f1_m: 0.7202\nEpoch 53/100\n10/10 [==============================] - 15s 1s/step - loss: 0.5306 - f1_m: 0.7707 - val_loss: 0.6473 - val_f1_m: 0.7344\nEpoch 54/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5143 - f1_m: 0.7666 - val_loss: 0.6271 - val_f1_m: 0.7000\nEpoch 55/100\n10/10 [==============================] - 15s 2s/step - loss: 0.5226 - f1_m: 0.7635 - val_loss: 0.6418 - val_f1_m: 0.7245\nEpoch 56/100\n10/10 [==============================] - 16s 2s/step - loss: 0.4893 - f1_m: 0.7806 - val_loss: 0.6546 - val_f1_m: 0.7476\nEpoch 56: early stopping\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**F1_Score is more than 75 now, so there is an improvment in precision score. Lets see the classification report**","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(xtest_pad)\n\n# Here '2' is '-1' in previous reports\n\nprint(classification_report(np.argmax(y_test_enc,axis=1), np.argmax(y_pred,axis=1), labels=[0,1,2]))   ","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:37:14.064483Z","iopub.execute_input":"2023-02-22T04:37:14.064788Z","iopub.status.idle":"2023-02-22T04:37:19.483554Z","shell.execute_reply.started":"2023-02-22T04:37:14.064756Z","shell.execute_reply":"2023-02-22T04:37:19.482691Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"37/37 [==============================] - 3s 68ms/step\n              precision    recall  f1-score   support\n\n           0       0.75      0.89      0.81       626\n           1       0.77      0.73      0.75       371\n           2       0.54      0.23      0.32       172\n\n    accuracy                           0.74      1169\n   macro avg       0.69      0.61      0.63      1169\nweighted avg       0.72      0.74      0.72      1169\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Its evident that there is a improvement in all the clasification classes, hence this will be our new baseline model.** ReduceLROnPlateau","metadata":{}},{"cell_type":"markdown","source":"##  Fasttext + early stopping","metadata":{}},{"cell_type":"code","source":"# Create embedding matrix for words that we have in dataset from fast text vectors\n\nembedding_matrix_fasttext = np.zeros((len(word_index)+1, 300))\nfor word,i in word_index.items():\n    try:\n        embedding_vector = embeddings.get_vector(word)\n        embedding_matrix_fasttext[i] = embedding_vector\n    except:\n        pass","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:38:03.536991Z","iopub.execute_input":"2023-02-22T04:38:03.537733Z","iopub.status.idle":"2023-02-22T04:38:03.578895Z","shell.execute_reply.started":"2023-02-22T04:38:03.537694Z","shell.execute_reply":"2023-02-22T04:38:03.578188Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix_fasttext],\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics = [f1_m_()])\n\n# Fit the model with early stopping callback + reduce lr callback\nearlystop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=5, min_lr=0.0001)\nhistory = model.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop,reduce_lr])","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:38:05.646614Z","iopub.execute_input":"2023-02-22T04:38:05.646878Z","iopub.status.idle":"2023-02-22T04:48:51.663596Z","shell.execute_reply.started":"2023-02-22T04:38:05.646849Z","shell.execute_reply":"2023-02-22T04:48:51.662943Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Epoch 1/100\n10/10 [==============================] - 19s 2s/step - loss: 1.0369 - f1_m: 0.1548 - val_loss: 0.9874 - val_f1_m: 0.0318 - lr: 0.0010\nEpoch 2/100\n10/10 [==============================] - 16s 2s/step - loss: 0.9642 - f1_m: 0.4253 - val_loss: 0.9357 - val_f1_m: 0.4333 - lr: 0.0010\nEpoch 3/100\n10/10 [==============================] - 16s 2s/step - loss: 0.9215 - f1_m: 0.4572 - val_loss: 0.9010 - val_f1_m: 0.4805 - lr: 0.0010\nEpoch 4/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8840 - f1_m: 0.5128 - val_loss: 0.8525 - val_f1_m: 0.5009 - lr: 0.0010\nEpoch 5/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8668 - f1_m: 0.4961 - val_loss: 0.8449 - val_f1_m: 0.5262 - lr: 0.0010\nEpoch 6/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8529 - f1_m: 0.5196 - val_loss: 0.8285 - val_f1_m: 0.5213 - lr: 0.0010\nEpoch 7/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8484 - f1_m: 0.5289 - val_loss: 0.8284 - val_f1_m: 0.5705 - lr: 0.0010\nEpoch 8/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8307 - f1_m: 0.5627 - val_loss: 0.8415 - val_f1_m: 0.5628 - lr: 0.0010\nEpoch 9/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8426 - f1_m: 0.5524 - val_loss: 0.8340 - val_f1_m: 0.5444 - lr: 0.0010\nEpoch 10/100\n10/10 [==============================] - 16s 2s/step - loss: 0.8414 - f1_m: 0.5557 - val_loss: 0.8144 - val_f1_m: 0.5939 - lr: 0.0010\nEpoch 11/100\n10/10 [==============================] - 15s 2s/step - loss: 0.8211 - f1_m: 0.5892 - val_loss: 0.8062 - val_f1_m: 0.5903 - lr: 0.0010\nEpoch 12/100\n10/10 [==============================] - 17s 2s/step - loss: 0.8030 - f1_m: 0.5799 - val_loss: 0.8069 - val_f1_m: 0.5752 - lr: 0.0010\nEpoch 13/100\n10/10 [==============================] - 15s 2s/step - loss: 0.7966 - f1_m: 0.5974 - val_loss: 0.8162 - val_f1_m: 0.5986 - lr: 0.0010\nEpoch 14/100\n10/10 [==============================] - 17s 2s/step - loss: 0.7851 - f1_m: 0.5973 - val_loss: 0.7788 - val_f1_m: 0.6022 - lr: 0.0010\nEpoch 15/100\n10/10 [==============================] - 15s 2s/step - loss: 0.7867 - f1_m: 0.6313 - val_loss: 0.7876 - val_f1_m: 0.6012 - lr: 0.0010\nEpoch 16/100\n10/10 [==============================] - 17s 2s/step - loss: 0.7707 - f1_m: 0.6333 - val_loss: 0.7602 - val_f1_m: 0.6487 - lr: 0.0010\nEpoch 17/100\n10/10 [==============================] - 16s 2s/step - loss: 0.7380 - f1_m: 0.6510 - val_loss: 0.7612 - val_f1_m: 0.6852 - lr: 0.0010\nEpoch 18/100\n10/10 [==============================] - 16s 2s/step - loss: 0.7412 - f1_m: 0.6608 - val_loss: 0.7419 - val_f1_m: 0.6667 - lr: 0.0010\nEpoch 19/100\n10/10 [==============================] - 16s 2s/step - loss: 0.7275 - f1_m: 0.6656 - val_loss: 0.7171 - val_f1_m: 0.6363 - lr: 0.0010\nEpoch 20/100\n10/10 [==============================] - 17s 2s/step - loss: 0.7050 - f1_m: 0.6665 - val_loss: 0.7067 - val_f1_m: 0.6681 - lr: 0.0010\nEpoch 21/100\n10/10 [==============================] - 16s 2s/step - loss: 0.7115 - f1_m: 0.6598 - val_loss: 0.7034 - val_f1_m: 0.6787 - lr: 0.0010\nEpoch 22/100\n10/10 [==============================] - 16s 2s/step - loss: 0.7006 - f1_m: 0.6616 - val_loss: 0.6810 - val_f1_m: 0.6941 - lr: 0.0010\nEpoch 23/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6794 - f1_m: 0.6876 - val_loss: 0.7095 - val_f1_m: 0.6673 - lr: 0.0010\nEpoch 24/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6613 - f1_m: 0.6897 - val_loss: 0.6742 - val_f1_m: 0.6653 - lr: 0.0010\nEpoch 25/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6580 - f1_m: 0.7020 - val_loss: 0.6560 - val_f1_m: 0.6956 - lr: 0.0010\nEpoch 26/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6247 - f1_m: 0.7132 - val_loss: 0.6618 - val_f1_m: 0.7020 - lr: 0.0010\nEpoch 27/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6227 - f1_m: 0.7175 - val_loss: 0.6583 - val_f1_m: 0.7085 - lr: 0.0010\nEpoch 28/100\n10/10 [==============================] - 16s 2s/step - loss: 0.6228 - f1_m: 0.7248 - val_loss: 0.6479 - val_f1_m: 0.6952 - lr: 0.0010\nEpoch 29/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5918 - f1_m: 0.7359 - val_loss: 0.6393 - val_f1_m: 0.7369 - lr: 0.0010\nEpoch 30/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5925 - f1_m: 0.7238 - val_loss: 0.6373 - val_f1_m: 0.7120 - lr: 0.0010\nEpoch 31/100\n10/10 [==============================] - 15s 2s/step - loss: 0.5850 - f1_m: 0.7342 - val_loss: 0.6598 - val_f1_m: 0.7074 - lr: 0.0010\nEpoch 32/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5561 - f1_m: 0.7523 - val_loss: 0.6468 - val_f1_m: 0.7107 - lr: 0.0010\nEpoch 33/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5680 - f1_m: 0.7413 - val_loss: 0.7023 - val_f1_m: 0.6995 - lr: 0.0010\nEpoch 34/100\n10/10 [==============================] - 17s 2s/step - loss: 0.5736 - f1_m: 0.7426 - val_loss: 0.6494 - val_f1_m: 0.6882 - lr: 0.0010\nEpoch 35/100\n10/10 [==============================] - 15s 2s/step - loss: 0.5847 - f1_m: 0.7293 - val_loss: 0.6872 - val_f1_m: 0.6899 - lr: 0.0010\nEpoch 36/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5589 - f1_m: 0.7462 - val_loss: 0.6516 - val_f1_m: 0.7236 - lr: 8.0000e-04\nEpoch 37/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5639 - f1_m: 0.7548 - val_loss: 0.6417 - val_f1_m: 0.7195 - lr: 8.0000e-04\nEpoch 38/100\n10/10 [==============================] - 17s 2s/step - loss: 0.5386 - f1_m: 0.7604 - val_loss: 0.6454 - val_f1_m: 0.7323 - lr: 8.0000e-04\nEpoch 39/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5191 - f1_m: 0.7702 - val_loss: 0.6666 - val_f1_m: 0.7370 - lr: 8.0000e-04\nEpoch 40/100\n10/10 [==============================] - 16s 2s/step - loss: 0.5002 - f1_m: 0.7763 - val_loss: 0.6520 - val_f1_m: 0.7136 - lr: 8.0000e-04\nEpoch 40: early stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = model.predict(xtest_pad)\n\n# Here '2' is '-1' in previous reports\n\nprint(classification_report(np.argmax(y_test_enc,axis=1), np.argmax(y_pred,axis=1), labels=[0,1,2]))   ","metadata":{"execution":{"iopub.status.busy":"2023-02-22T04:49:35.381752Z","iopub.execute_input":"2023-02-22T04:49:35.382028Z","iopub.status.idle":"2023-02-22T04:50:33.488839Z","shell.execute_reply.started":"2023-02-22T04:49:35.381998Z","shell.execute_reply":"2023-02-22T04:50:33.487556Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"37/37 [==============================] - 3s 66ms/step\n              precision    recall  f1-score   support\n\n           0       0.77      0.81      0.79       626\n           1       0.69      0.78      0.73       371\n           2       0.43      0.23      0.30       172\n\n    accuracy                           0.72      1169\n   macro avg       0.63      0.61      0.61      1169\nweighted avg       0.70      0.72      0.70      1169\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model 10 bidirectional LSTM ","metadata":{}},{"cell_type":"code","source":"model_bi = Sequential()\nmodel_bi.add(Embedding(len(word_index)+1, 300, weights=[embedding_matrix], trainable = False))\n\nmodel_bi.add(SpatialDropout1D(0.3))\nmodel_bi.add(Bidirectional(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3)))\n\nmodel_bi.add(Dense(1024, activation = 'relu'))\nmodel_bi.add(Dropout(0.8))\n\nmodel_bi.add(Dense(1024, activation = 'relu'))\nmodel_bi.add(Dropout(0.8))\n\nmodel_bi.add(Dense(3))\nmodel_bi.add(Activation('softmax'))\nmodel.summary()\n\nmodel_bi.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = f1_m_())\n\nearlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 10, verbose = 1, mode = 'auto')\nhistory = model_bi.fit(xtrain_pad, y=y_train_enc, batch_size = 128, epochs = 100, verbose=1, validation_data = (xtest_pad, y_test_enc),callbacks = [earlystop])\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:20:33.383041Z","iopub.execute_input":"2023-02-22T05:20:33.383816Z","iopub.status.idle":"2023-02-22T05:44:12.647444Z","shell.execute_reply.started":"2023-02-22T05:20:33.383779Z","shell.execute_reply":"2023-02-22T05:44:12.646739Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_3 (Embedding)     (None, None, 300)         2916000   \n                                                                 \n spatial_dropout1d_3 (Spatia  (None, None, 300)        0         \n lDropout1D)                                                     \n                                                                 \n lstm_3 (LSTM)               (None, 300)               721200    \n                                                                 \n dense_9 (Dense)             (None, 1024)              308224    \n                                                                 \n dropout_6 (Dropout)         (None, 1024)              0         \n                                                                 \n dense_10 (Dense)            (None, 1024)              1049600   \n                                                                 \n dropout_7 (Dropout)         (None, 1024)              0         \n                                                                 \n dense_11 (Dense)            (None, 3)                 3075      \n                                                                 \n activation_3 (Activation)   (None, 3)                 0         \n                                                                 \n=================================================================\nTotal params: 4,998,099\nTrainable params: 2,082,099\nNon-trainable params: 2,916,000\n_________________________________________________________________\nEpoch 1/100\n37/37 [==============================] - 55s 1s/step - loss: 0.9579 - f1_m: 0.4154 - val_loss: 0.8555 - val_f1_m: 0.5448\nEpoch 2/100\n37/37 [==============================] - 50s 1s/step - loss: 0.8589 - f1_m: 0.5256 - val_loss: 0.8035 - val_f1_m: 0.5580\nEpoch 3/100\n37/37 [==============================] - 50s 1s/step - loss: 0.8082 - f1_m: 0.5947 - val_loss: 0.7620 - val_f1_m: 0.5832\nEpoch 4/100\n37/37 [==============================] - 50s 1s/step - loss: 0.7764 - f1_m: 0.6175 - val_loss: 0.7530 - val_f1_m: 0.6373\nEpoch 5/100\n37/37 [==============================] - 50s 1s/step - loss: 0.7528 - f1_m: 0.6314 - val_loss: 0.7077 - val_f1_m: 0.6473\nEpoch 6/100\n37/37 [==============================] - 50s 1s/step - loss: 0.7373 - f1_m: 0.6498 - val_loss: 0.7029 - val_f1_m: 0.6297\nEpoch 7/100\n37/37 [==============================] - 50s 1s/step - loss: 0.7145 - f1_m: 0.6548 - val_loss: 0.6924 - val_f1_m: 0.6401\nEpoch 8/100\n37/37 [==============================] - 49s 1s/step - loss: 0.6918 - f1_m: 0.6671 - val_loss: 0.6673 - val_f1_m: 0.6851\nEpoch 9/100\n37/37 [==============================] - 50s 1s/step - loss: 0.6764 - f1_m: 0.6848 - val_loss: 0.6547 - val_f1_m: 0.6812\nEpoch 10/100\n37/37 [==============================] - 50s 1s/step - loss: 0.6514 - f1_m: 0.6960 - val_loss: 0.6514 - val_f1_m: 0.6931\nEpoch 11/100\n37/37 [==============================] - 51s 1s/step - loss: 0.6315 - f1_m: 0.7113 - val_loss: 0.6194 - val_f1_m: 0.6952\nEpoch 12/100\n37/37 [==============================] - 51s 1s/step - loss: 0.6034 - f1_m: 0.7237 - val_loss: 0.6227 - val_f1_m: 0.6902\nEpoch 13/100\n37/37 [==============================] - 49s 1s/step - loss: 0.5855 - f1_m: 0.7273 - val_loss: 0.6242 - val_f1_m: 0.6839\nEpoch 14/100\n37/37 [==============================] - 51s 1s/step - loss: 0.5780 - f1_m: 0.7354 - val_loss: 0.6197 - val_f1_m: 0.6876\nEpoch 15/100\n37/37 [==============================] - 50s 1s/step - loss: 0.5580 - f1_m: 0.7403 - val_loss: 0.6131 - val_f1_m: 0.6948\nEpoch 16/100\n37/37 [==============================] - 51s 1s/step - loss: 0.5525 - f1_m: 0.7509 - val_loss: 0.6400 - val_f1_m: 0.6896\nEpoch 17/100\n37/37 [==============================] - 50s 1s/step - loss: 0.5327 - f1_m: 0.7516 - val_loss: 0.6213 - val_f1_m: 0.6785\nEpoch 18/100\n37/37 [==============================] - 51s 1s/step - loss: 0.5277 - f1_m: 0.7547 - val_loss: 0.5873 - val_f1_m: 0.7227\nEpoch 19/100\n37/37 [==============================] - 51s 1s/step - loss: 0.5020 - f1_m: 0.7676 - val_loss: 0.6106 - val_f1_m: 0.7115\nEpoch 20/100\n37/37 [==============================] - 50s 1s/step - loss: 0.4799 - f1_m: 0.7731 - val_loss: 0.5939 - val_f1_m: 0.6863\nEpoch 21/100\n37/37 [==============================] - 51s 1s/step - loss: 0.4735 - f1_m: 0.7790 - val_loss: 0.6139 - val_f1_m: 0.6988\nEpoch 22/100\n37/37 [==============================] - 51s 1s/step - loss: 0.4568 - f1_m: 0.7871 - val_loss: 0.6008 - val_f1_m: 0.6900\nEpoch 23/100\n37/37 [==============================] - 51s 1s/step - loss: 0.4503 - f1_m: 0.7994 - val_loss: 0.6221 - val_f1_m: 0.7056\nEpoch 24/100\n37/37 [==============================] - 52s 1s/step - loss: 0.4352 - f1_m: 0.7994 - val_loss: 0.6229 - val_f1_m: 0.7007\nEpoch 25/100\n37/37 [==============================] - 50s 1s/step - loss: 0.4219 - f1_m: 0.8096 - val_loss: 0.6178 - val_f1_m: 0.7052\nEpoch 26/100\n37/37 [==============================] - 51s 1s/step - loss: 0.4258 - f1_m: 0.7996 - val_loss: 0.6691 - val_f1_m: 0.7131\nEpoch 27/100\n37/37 [==============================] - 54s 1s/step - loss: 0.4199 - f1_m: 0.8066 - val_loss: 0.6706 - val_f1_m: 0.7080\nEpoch 28/100\n37/37 [==============================] - 50s 1s/step - loss: 0.3932 - f1_m: 0.8181 - val_loss: 0.7010 - val_f1_m: 0.6730\nEpoch 28: early stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = model_bi.predict(xtest_pad)\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred , axis=1), labels = [0,1,2]))","metadata":{"execution":{"iopub.status.busy":"2023-02-22T06:12:52.288406Z","iopub.execute_input":"2023-02-22T06:12:52.289035Z","iopub.status.idle":"2023-02-22T06:13:02.576957Z","shell.execute_reply.started":"2023-02-22T06:12:52.288995Z","shell.execute_reply":"2023-02-22T06:13:02.576177Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"37/37 [==============================] - 5s 140ms/step\n              precision    recall  f1-score   support\n\n           0       0.77      0.73      0.75       626\n           1       0.69      0.77      0.73       371\n           2       0.38      0.36      0.37       172\n\n    accuracy                           0.69      1169\n   macro avg       0.61      0.62      0.62      1169\nweighted avg       0.69      0.69      0.69      1169\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Well bidrectional LSTM beat our baseline LSTM model so we are going to consider this model**","metadata":{}},{"cell_type":"markdown","source":"# Model 11 GRU 2 layers","metadata":{}},{"cell_type":"code","source":"model_gru = Sequential()\nmodel_gru.add(Embedding(len(word_index)+1, 300, weights = [embedding_matrix], trainable=False))\n\nmodel_gru.add(SpatialDropout1D(0.3))\nmodel_gru.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences = True))\nmodel_gru.add(GRU(300, dropout = 0.3, recurrent_dropout = 0.3))\n\nmodel_gru.add(Dense(1024, activation = 'relu'))\nmodel_gru.add(Dropout(0.8))\n\nmodel_gru.add(Dense(1024, activation = 'relu'))\nmodel_gru.add(Dropout(0.8))\n\nmodel_gru.add(Dense(3, activation = 'softmax'))\nmodel_gru.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = f1_m_())\n\nearlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 10, verbose = 1, mode= 'auto')\n\nmodel_gru.fit(xtrain_pad, y=y_train_enc, batch_size = 512, epochs = 100, verbose=1, validation_data = (xtest_pad, y_test_enc),callbacks = [earlystop])","metadata":{"execution":{"iopub.status.busy":"2023-02-21T12:53:03.951733Z","iopub.execute_input":"2023-02-21T12:53:03.952048Z","iopub.status.idle":"2023-02-21T12:59:45.227255Z","shell.execute_reply.started":"2023-02-21T12:53:03.952017Z","shell.execute_reply":"2023-02-21T12:59:45.226483Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Epoch 1/100\n10/10 [==============================] - 32s 3s/step - loss: 1.0459 - f1_m: 0.2057 - val_loss: 0.9980 - val_f1_m: 0.0046\nEpoch 2/100\n10/10 [==============================] - 26s 3s/step - loss: 0.9961 - f1_m: 0.1690 - val_loss: 0.9856 - val_f1_m: 0.5443\nEpoch 3/100\n10/10 [==============================] - 27s 3s/step - loss: 0.9906 - f1_m: 0.4777 - val_loss: 0.9839 - val_f1_m: 0.5438\nEpoch 4/100\n10/10 [==============================] - 26s 3s/step - loss: 0.9833 - f1_m: 0.4639 - val_loss: 0.9816 - val_f1_m: 0.5443\nEpoch 5/100\n10/10 [==============================] - 26s 3s/step - loss: 0.9858 - f1_m: 0.4815 - val_loss: 0.9830 - val_f1_m: 0.5435\nEpoch 6/100\n10/10 [==============================] - 27s 3s/step - loss: 0.9870 - f1_m: 0.3737 - val_loss: 0.9801 - val_f1_m: 0.5443\nEpoch 7/100\n10/10 [==============================] - 26s 3s/step - loss: 0.9846 - f1_m: 0.4930 - val_loss: 0.9800 - val_f1_m: 0.5443\nEpoch 8/100\n10/10 [==============================] - 27s 3s/step - loss: 0.9841 - f1_m: 0.4714 - val_loss: 0.9786 - val_f1_m: 0.5443\nEpoch 9/100\n10/10 [==============================] - 26s 3s/step - loss: 0.9811 - f1_m: 0.4594 - val_loss: 0.9566 - val_f1_m: 0.3931\nEpoch 10/100\n10/10 [==============================] - 26s 3s/step - loss: 0.9547 - f1_m: 0.4487 - val_loss: 0.9010 - val_f1_m: 0.4884\nEpoch 11/100\n10/10 [==============================] - 27s 3s/step - loss: 0.9168 - f1_m: 0.4836 - val_loss: 0.8921 - val_f1_m: 0.5300\nEpoch 12/100\n10/10 [==============================] - 27s 3s/step - loss: 0.8982 - f1_m: 0.5073 - val_loss: 0.8640 - val_f1_m: 0.5083\nEpoch 13/100\n10/10 [==============================] - 26s 3s/step - loss: 0.8838 - f1_m: 0.4885 - val_loss: 0.8643 - val_f1_m: 0.5090\nEpoch 14/100\n10/10 [==============================] - 27s 3s/step - loss: 0.8799 - f1_m: 0.5008 - val_loss: 0.8776 - val_f1_m: 0.5517\nEpoch 15/100\n10/10 [==============================] - 26s 3s/step - loss: 0.9149 - f1_m: 0.4993 - val_loss: 0.9011 - val_f1_m: 0.4188\n","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fe10c7048d0>"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = model_gru.predict(xtest_pad)\n\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred , axis=1), labels = [0,1,2]))","metadata":{"execution":{"iopub.status.busy":"2023-02-21T13:01:16.113714Z","iopub.execute_input":"2023-02-21T13:01:16.114068Z","iopub.status.idle":"2023-02-21T13:01:21.12481Z","shell.execute_reply.started":"2023-02-21T13:01:16.11403Z","shell.execute_reply":"2023-02-21T13:01:21.12408Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"37/37 [==============================] - 5s 113ms/step\n              precision    recall  f1-score   support\n\n           0       0.78      0.67      0.72       626\n           1       0.43      0.73      0.54       371\n           2       0.00      0.00      0.00       172\n\n    accuracy                           0.59      1169\n   macro avg       0.40      0.47      0.42      1169\nweighted avg       0.55      0.59      0.56      1169\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Well not much difference between baseline single layer LSTM model to GRU model, but it looks our baseline model is still better.**\n\n**Lets add one more LSTM layer to baseline model and see what happens**","metadata":{}},{"cell_type":"markdown","source":"# Model 12 LSTM multiple layers","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3 ,  return_sequences = True))\nmodel.add(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics = [f1_m_()])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\nmodel.fit(xtrain_pad, y=y_train_enc, batch_size=128, epochs=100, \n          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])\n\ny_pred = model.predict(xtest_pad)\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred, axis=1), labels=[0,1,2]))","metadata":{"execution":{"iopub.status.busy":"2023-02-22T05:44:18.388662Z","iopub.execute_input":"2023-02-22T05:44:18.38893Z","iopub.status.idle":"2023-02-22T06:12:52.286628Z","shell.execute_reply.started":"2023-02-22T05:44:18.388895Z","shell.execute_reply":"2023-02-22T06:12:52.285725Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Epoch 1/100\n37/37 [==============================] - 59s 1s/step - loss: 0.9868 - f1_m: 0.3155 - val_loss: 0.9125 - val_f1_m: 0.4896\nEpoch 2/100\n37/37 [==============================] - 54s 1s/step - loss: 0.9055 - f1_m: 0.4938 - val_loss: 0.8551 - val_f1_m: 0.5359\nEpoch 3/100\n37/37 [==============================] - 53s 1s/step - loss: 0.8757 - f1_m: 0.5137 - val_loss: 0.8438 - val_f1_m: 0.5751\nEpoch 4/100\n37/37 [==============================] - 53s 1s/step - loss: 0.8698 - f1_m: 0.5229 - val_loss: 0.8298 - val_f1_m: 0.5156\nEpoch 5/100\n37/37 [==============================] - 56s 2s/step - loss: 0.8674 - f1_m: 0.5272 - val_loss: 0.8222 - val_f1_m: 0.5781\nEpoch 6/100\n37/37 [==============================] - 53s 1s/step - loss: 0.8630 - f1_m: 0.5425 - val_loss: 0.8237 - val_f1_m: 0.5319\nEpoch 7/100\n37/37 [==============================] - 53s 1s/step - loss: 0.8407 - f1_m: 0.5484 - val_loss: 0.7804 - val_f1_m: 0.6157\nEpoch 8/100\n37/37 [==============================] - 53s 1s/step - loss: 0.8055 - f1_m: 0.5918 - val_loss: 0.7639 - val_f1_m: 0.6286\nEpoch 9/100\n37/37 [==============================] - 53s 1s/step - loss: 0.7938 - f1_m: 0.6006 - val_loss: 0.7841 - val_f1_m: 0.5721\nEpoch 10/100\n37/37 [==============================] - 53s 1s/step - loss: 0.7595 - f1_m: 0.6264 - val_loss: 0.7762 - val_f1_m: 0.5796\nEpoch 11/100\n37/37 [==============================] - 54s 1s/step - loss: 0.7452 - f1_m: 0.6423 - val_loss: 0.7240 - val_f1_m: 0.6898\nEpoch 12/100\n37/37 [==============================] - 53s 1s/step - loss: 0.7187 - f1_m: 0.6506 - val_loss: 0.6586 - val_f1_m: 0.6846\nEpoch 13/100\n37/37 [==============================] - 53s 1s/step - loss: 0.7011 - f1_m: 0.6728 - val_loss: 0.6516 - val_f1_m: 0.6918\nEpoch 14/100\n37/37 [==============================] - 53s 1s/step - loss: 0.6767 - f1_m: 0.6911 - val_loss: 0.6283 - val_f1_m: 0.6992\nEpoch 15/100\n37/37 [==============================] - 54s 1s/step - loss: 0.6617 - f1_m: 0.6891 - val_loss: 0.6706 - val_f1_m: 0.6871\nEpoch 16/100\n37/37 [==============================] - 53s 1s/step - loss: 0.6588 - f1_m: 0.6918 - val_loss: 0.6371 - val_f1_m: 0.6925\nEpoch 17/100\n37/37 [==============================] - 52s 1s/step - loss: 0.6365 - f1_m: 0.7045 - val_loss: 0.6159 - val_f1_m: 0.7007\nEpoch 18/100\n37/37 [==============================] - 56s 2s/step - loss: 0.6058 - f1_m: 0.7180 - val_loss: 0.6185 - val_f1_m: 0.7196\nEpoch 19/100\n37/37 [==============================] - 53s 1s/step - loss: 0.5894 - f1_m: 0.7327 - val_loss: 0.6250 - val_f1_m: 0.7123\nEpoch 20/100\n37/37 [==============================] - 52s 1s/step - loss: 0.5821 - f1_m: 0.7277 - val_loss: 0.6282 - val_f1_m: 0.6760\nEpoch 21/100\n37/37 [==============================] - 54s 1s/step - loss: 0.5503 - f1_m: 0.7527 - val_loss: 0.6216 - val_f1_m: 0.6906\nEpoch 22/100\n37/37 [==============================] - 54s 1s/step - loss: 0.5455 - f1_m: 0.7569 - val_loss: 0.6056 - val_f1_m: 0.7088\nEpoch 23/100\n37/37 [==============================] - 53s 1s/step - loss: 0.5513 - f1_m: 0.7487 - val_loss: 0.6106 - val_f1_m: 0.7134\nEpoch 24/100\n37/37 [==============================] - 53s 1s/step - loss: 0.5271 - f1_m: 0.7528 - val_loss: 0.6374 - val_f1_m: 0.6967\nEpoch 25/100\n37/37 [==============================] - 53s 1s/step - loss: 0.5345 - f1_m: 0.7528 - val_loss: 0.6673 - val_f1_m: 0.7241\nEpoch 26/100\n37/37 [==============================] - 52s 1s/step - loss: 0.5230 - f1_m: 0.7670 - val_loss: 0.6368 - val_f1_m: 0.7239\nEpoch 27/100\n37/37 [==============================] - 53s 1s/step - loss: 0.5191 - f1_m: 0.7674 - val_loss: 0.6460 - val_f1_m: 0.6861\nEpoch 28/100\n37/37 [==============================] - 53s 1s/step - loss: 0.4988 - f1_m: 0.7784 - val_loss: 0.6350 - val_f1_m: 0.7146\nEpoch 29/100\n37/37 [==============================] - 53s 1s/step - loss: 0.4718 - f1_m: 0.7844 - val_loss: 0.7181 - val_f1_m: 0.7225\nEpoch 30/100\n37/37 [==============================] - 53s 1s/step - loss: 0.4815 - f1_m: 0.7802 - val_loss: 0.6663 - val_f1_m: 0.7014\nEpoch 31/100\n37/37 [==============================] - 52s 1s/step - loss: 0.4793 - f1_m: 0.7818 - val_loss: 0.6231 - val_f1_m: 0.7109\nEpoch 32/100\n37/37 [==============================] - 52s 1s/step - loss: 0.4516 - f1_m: 0.7952 - val_loss: 0.6457 - val_f1_m: 0.6847\nEpoch 32: early stopping\n37/37 [==============================] - 6s 158ms/step\n              precision    recall  f1-score   support\n\n           0       0.78      0.72      0.75       626\n           1       0.65      0.84      0.73       371\n           2       0.42      0.27      0.33       172\n\n    accuracy                           0.69      1169\n   macro avg       0.62      0.61      0.60      1169\nweighted avg       0.69      0.69      0.68      1169\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Well the additional LSTM layer has imporoved the first 2 labels and did not imporove much for third label**","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\n#model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3 ,  return_sequences = True))\n#model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3 ,  return_sequences = True))\nmodel.add(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3))\n\nmodel.add(Dense(1024, activation='relu'))\n#model.add(Dropout(0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics = [f1_m_()])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])\n\ny_pred = model.predict(xtest_pad)\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred, axis=1), labels=[0,1,2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     trainable=False))\n#model.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, recurrent_dropout = 0.1))\n\nmodel.add(Dense(1024, activation='relu'))\n#model.add(Dropout(0.3))\nmodel.add(Dense(1024, activation='relu'))\n\nmodel.add(Dense(512, activation='relu'))\n#model.add(Dropout(0.3))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics = [f1_m_()])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])\n\ny_pred = model.predict(xtest_pad)\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred, axis=1), labels=[0,1,2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     trainable=False))\n#model.add(SpatialDropout1D(0.3))\n#model.add(LSTM(300, recurrent_dropout = 0.1,return_sequences = True ))\nmodel.add(LSTM(300, recurrent_dropout = 0.1))\n\nmodel.add(Dense(2000, activation='relu'))\n\nmodel.add(Dense(2000, activation='relu'))\n#model.add(Dropout(0.3))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dense(750, activation='relu'))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics = [f1_m_()])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])\n\ny_pred = model.predict(xtest_pad)\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred, axis=1), labels=[0,1,2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This model works better for the label 2 class** ","metadata":{}},{"cell_type":"markdown","source":"# Classify text with HuggingFace Pipelines Transformers\n","metadata":{}},{"cell_type":"code","source":"classifier = pipeline(\"text-classification\", model=\"j-hartmann/sentiment-roberta-large-english-3-classes\")\n","metadata":{"execution":{"iopub.status.busy":"2023-02-23T04:03:24.615544Z","iopub.execute_input":"2023-02-23T04:03:24.61582Z","iopub.status.idle":"2023-02-23T04:04:41.837834Z","shell.execute_reply.started":"2023-02-23T04:03:24.615788Z","shell.execute_reply":"2023-02-23T04:04:41.836971Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/725 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032c1b1dfa9949c287eb7b4080c0f51b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c20d6151defd4caf90cc762ab675de6a"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at j-hartmann/sentiment-roberta-large-english-3-classes were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba4a0ecec9e649b48ff7746055176a50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b70cf88ca6f340c6a7e7a4a48e23646a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faff7de34ec149d6ae32e36b28e308bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbaee245af9e485fa4d157e2bf88f0bc"}},"metadata":{}}]},{"cell_type":"code","source":"output = classifier( df.Sentence.to_list() )","metadata":{"execution":{"iopub.status.busy":"2023-02-22T06:23:34.543757Z","iopub.execute_input":"2023-02-22T06:23:34.544025Z","iopub.status.idle":"2023-02-22T07:06:47.681648Z","shell.execute_reply.started":"2023-02-22T06:23:34.543995Z","shell.execute_reply":"2023-02-22T07:06:47.680761Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"y_pred_pipeline = []\nfor i in output:\n    y_pred_pipeline.append(i['label'])","metadata":{"execution":{"iopub.status.busy":"2023-02-22T07:39:03.594844Z","iopub.execute_input":"2023-02-22T07:39:03.59511Z","iopub.status.idle":"2023-02-22T07:39:03.600567Z","shell.execute_reply.started":"2023-02-22T07:39:03.59508Z","shell.execute_reply":"2023-02-22T07:39:03.59983Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"y_pred_pipeline = [dicto[i] for i in y_pred_pipeline]","metadata":{"execution":{"iopub.status.busy":"2023-02-22T07:48:42.92585Z","iopub.execute_input":"2023-02-22T07:48:42.92612Z","iopub.status.idle":"2023-02-22T07:48:42.931919Z","shell.execute_reply.started":"2023-02-22T07:48:42.926089Z","shell.execute_reply":"2023-02-22T07:48:42.931116Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"print(classification_report(df.Sentiment,y_pred_pipeline))","metadata":{"execution":{"iopub.status.busy":"2023-02-22T07:48:59.255585Z","iopub.execute_input":"2023-02-22T07:48:59.255847Z","iopub.status.idle":"2023-02-22T07:48:59.274318Z","shell.execute_reply.started":"2023-02-22T07:48:59.255818Z","shell.execute_reply":"2023-02-22T07:48:59.273397Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n          -1       0.54      0.11      0.19       860\n           0       0.55      0.97      0.70      3130\n           1       0.76      0.04      0.07      1852\n\n    accuracy                           0.55      5842\n   macro avg       0.62      0.37      0.32      5842\nweighted avg       0.61      0.55      0.43      5842\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### This method is So bad","metadata":{}},{"cell_type":"markdown","source":"# Classify text with BERT ( Transfer Learning )\n**this model needs more memory to run**","metadata":{}},{"cell_type":"code","source":"# Load the pre-trained BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nbert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=3)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T04:47:35.760447Z","iopub.execute_input":"2023-02-23T04:47:35.760718Z","iopub.status.idle":"2023-02-23T04:47:43.275088Z","shell.execute_reply.started":"2023-02-23T04:47:35.760686Z","shell.execute_reply":"2023-02-23T04:47:43.274327Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stderr","text":"All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-23T04:47:43.276733Z","iopub.execute_input":"2023-02-23T04:47:43.277349Z","iopub.status.idle":"2023-02-23T04:47:43.310675Z","shell.execute_reply.started":"2023-02-23T04:47:43.27731Z","shell.execute_reply":"2023-02-23T04:47:43.309985Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Model: \"tf_bert_for_sequence_classification_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bert (TFBertMainLayer)      multiple                  109482240 \n                                                                 \n dropout_227 (Dropout)       multiple                  0         \n                                                                 \n classifier (Dense)          multiple                  2307      \n                                                                 \n=================================================================\nTotal params: 109,484,547\nTrainable params: 109,484,547\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the optimizer, loss function, and metrics\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\nloss = tf.keras.losses.CategoricalCrossentropy()\nmetrics = [tf.keras.metrics.CategoricalAccuracy()]","metadata":{"execution":{"iopub.status.busy":"2023-02-23T04:58:46.968957Z","iopub.execute_input":"2023-02-23T04:58:46.969234Z","iopub.status.idle":"2023-02-23T04:58:46.976112Z","shell.execute_reply.started":"2023-02-23T04:58:46.969203Z","shell.execute_reply":"2023-02-23T04:58:46.975352Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"# Define the model inputs and outputs\ninput_ids = tf.keras.layers.Input(shape=(98,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(98,), dtype=tf.int32, name='attention_mask')\noutput = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})[0]\n\n# Add a dense layer with softmax activation for classification\noutput = tf.keras.layers.Dense(3, activation='softmax')(output)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T04:59:05.987094Z","iopub.execute_input":"2023-02-23T04:59:05.987369Z","iopub.status.idle":"2023-02-23T04:59:07.648263Z","shell.execute_reply.started":"2023-02-23T04:59:05.98734Z","shell.execute_reply":"2023-02-23T04:59:07.647517Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"# Define the model\nmodel = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n# Compile the model\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-23T04:59:07.649641Z","iopub.execute_input":"2023-02-23T04:59:07.649886Z","iopub.status.idle":"2023-02-23T04:59:07.669841Z","shell.execute_reply.started":"2023-02-23T04:59:07.649847Z","shell.execute_reply":"2023-02-23T04:59:07.669196Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-23T04:59:08.191724Z","iopub.execute_input":"2023-02-23T04:59:08.192429Z","iopub.status.idle":"2023-02-23T04:59:08.235097Z","shell.execute_reply.started":"2023-02-23T04:59:08.192391Z","shell.execute_reply":"2023-02-23T04:59:08.234256Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"Model: \"model_5\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n attention_mask (InputLayer)    [(None, 98)]         0           []                               \n                                                                                                  \n input_ids (InputLayer)         [(None, 98)]         0           []                               \n                                                                                                  \n tf_bert_for_sequence_classific  TFSequenceClassifie  109484547  ['attention_mask[0][0]',         \n ation_5 (TFBertForSequenceClas  rOutput(loss=None,               'input_ids[0][0]']              \n sification)                    logits=(None, 3),                                                 \n                                 hidden_states=None                                               \n                                , attentions=None)                                                \n                                                                                                  \n dense_6 (Dense)                (None, 3)            12          ['tf_bert_for_sequence_classifica\n                                                                 tion_5[3][0]']                   \n                                                                                                  \n==================================================================================================\nTotal params: 109,484,559\nTrainable params: 109,484,559\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the training and validation datasets\n\ndef datasets(tokenizer):\n    \n    # Split the data into training and validation sets\n    train_df, val_df, train_labels, val_labels = train_test_split(df.Sentence, df.Sentiment,\n                                                                  test_size=0.2, random_state=42,stratify = df.Sentiment)\n    \n    # Tokenize the input sequences and convert to input IDs and attention masks\n    train_encodings = tokenizer(list(train_df.values), truncation=True, padding=True,max_length=98)\n    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=3)\n    train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']}, train_labels))\n    \n    val_encodings = tokenizer(list(val_df.values), truncation=True, padding=True,max_length=98)\n    val_labels = tf.keras.utils.to_categorical(val_labels, num_classes=3)\n    val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask']}, val_labels))\n    \n    \n    # Batch and shuffle the datasets\n    batch_size = 32\n    train_dataset = train_dataset.batch(batch_size).shuffle(1000)\n    val_dataset = val_dataset.batch(batch_size)\n    \n    return train_dataset,val_dataset","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:52:49.951356Z","iopub.execute_input":"2023-02-23T10:52:49.951649Z","iopub.status.idle":"2023-02-23T10:52:49.961057Z","shell.execute_reply.started":"2023-02-23T10:52:49.951617Z","shell.execute_reply":"2023-02-23T10:52:49.959495Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define the training and validation datasets\n\ntrain_dataset,val_dataset = datasets(tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model for a few epochs\nnum_epochs = 3\nmodel.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T04:59:15.564704Z","iopub.execute_input":"2023-02-23T04:59:15.564976Z","iopub.status.idle":"2023-02-23T07:33:47.99015Z","shell.execute_reply.started":"2023-02-23T04:59:15.564938Z","shell.execute_reply":"2023-02-23T07:33:47.988771Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"Epoch 1/3\n147/147 [==============================] - 3102s 21s/step - loss: 0.7851 - accuracy: 0.6405 - val_loss: 0.5857 - val_accuracy: 0.7562\nEpoch 2/3\n147/147 [==============================] - 3062s 21s/step - loss: 0.4691 - accuracy: 0.8095 - val_loss: 0.4511 - val_accuracy: 0.8024\nEpoch 3/3\n147/147 [==============================] - 3072s 21s/step - loss: 0.3209 - accuracy: 0.8566 - val_loss: 0.4699 - val_accuracy: 0.8041\n","output_type":"stream"},{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f0e4cdbbc50>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Classify text with RoBERTa ( Transfer Learning )\n**this model needs more memory to run**","metadata":{}},{"cell_type":"code","source":"# Load the RoBERTa tokenizer and model\nroberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nroberta_model = TFRobertaModel.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2023-02-23T07:59:37.431596Z","iopub.execute_input":"2023-02-23T07:59:37.431894Z","iopub.status.idle":"2023-02-23T07:59:44.845105Z","shell.execute_reply.started":"2023-02-23T07:59:37.431861Z","shell.execute_reply":"2023-02-23T07:59:44.844326Z"},"trusted":true},"execution_count":140,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the training and validation datasets\n\ntrain_dataset,val_dataset = datasets(roberta_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T08:00:04.912117Z","iopub.execute_input":"2023-02-23T08:00:04.912711Z","iopub.status.idle":"2023-02-23T08:00:09.567095Z","shell.execute_reply.started":"2023-02-23T08:00:04.912672Z","shell.execute_reply":"2023-02-23T08:00:09.566302Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"\n# Define the input shape for the model\nmax_length = 98\n\n# Define the custom top layer for classification\nnum_labels = 3\ntop_layer = tf.keras.layers.Dense(num_labels, activation='softmax')\n\n# Define the RoBERTa model with the custom top layer\ninput_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\nroberta_output = roberta_model({'input_ids': input_ids, 'attention_mask': attention_mask})\nroberta_output = roberta_output.last_hidden_state[:, 0, :]\nroberta_output = top_layer(roberta_output)\nroberta_model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=[roberta_output])\n\nprint(roberta_model.summary())\n\n# Define the loss function and metrics for training\nloss = tf.keras.losses.CategoricalCrossentropy()\nmetrics = [tf.keras.metrics.CategoricalAccuracy()]\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n\n# Compile the RoBERTa model for training\nroberta_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n\n# Train the RoBERTa model on the sentiment analysis task\nroberta_model.fit(train_dataset, epochs=3, validation_data=val_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-23T08:00:09.569136Z","iopub.execute_input":"2023-02-23T08:00:09.569393Z","iopub.status.idle":"2023-02-23T10:37:43.763606Z","shell.execute_reply.started":"2023-02-23T08:00:09.569359Z","shell.execute_reply":"2023-02-23T10:37:43.762827Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"Model: \"model_9\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n attention_mask (InputLayer)    [(None, 98)]         0           []                               \n                                                                                                  \n input_ids (InputLayer)         [(None, 98)]         0           []                               \n                                                                                                  \n tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  124645632  ['attention_mask[0][0]',         \n odel)                          thPoolingAndCrossAt               'input_ids[0][0]']              \n                                tentions(last_hidde                                               \n                                n_state=(None, 98,                                                \n                                768),                                                             \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n icingOpLambda)                                                                                   \n                                                                                                  \n dense_26 (Dense)               (None, 3)            2307        ['tf.__operators__.getitem_3[0][0\n                                                                 ]']                              \n                                                                                                  \n==================================================================================================\nTotal params: 124,647,939\nTrainable params: 124,647,939\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nEpoch 1/3\n147/147 [==============================] - 3186s 21s/step - loss: 0.6629 - categorical_accuracy: 0.6936 - val_loss: 0.4382 - val_categorical_accuracy: 0.7956\nEpoch 2/3\n147/147 [==============================] - 3081s 21s/step - loss: 0.3732 - categorical_accuracy: 0.8220 - val_loss: 0.5829 - val_categorical_accuracy: 0.7776\nEpoch 3/3\n147/147 [==============================] - 3098s 21s/step - loss: 0.2897 - categorical_accuracy: 0.8538 - val_loss: 0.4137 - val_categorical_accuracy: 0.8178\n","output_type":"stream"},{"execution_count":144,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f0e2f9fce50>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Classify text with GPT-2 ( Transfer Learning )\n**this model needs more memory to run**","metadata":{}},{"cell_type":"code","source":"# Load the GPT-2 tokenizer and model\ngpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ngpt2_model = TFGPT2Model.from_pretrained('gpt2')","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:52:59.103613Z","iopub.execute_input":"2023-02-23T10:52:59.103873Z","iopub.status.idle":"2023-02-23T10:53:21.266744Z","shell.execute_reply.started":"2023-02-23T10:52:59.103847Z","shell.execute_reply":"2023-02-23T10:53:21.26587Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecb9f04e0c9447109262cfc2134bc187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77f4d61bb19249c5877a734c2948c2d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37d2d2efb720465b87b995c36a9edec1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01b438d86d554853841ce615aa52e472"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/475M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1916b481e28e4566b31984a341c57581"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFGPT2Model.\n\nAll the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the training and validation datasets\n\n# Set pad_token\ngpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n\ntrain_dataset,val_dataset = datasets(gpt2_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:53:21.268415Z","iopub.execute_input":"2023-02-23T10:53:21.268717Z","iopub.status.idle":"2023-02-23T10:53:26.25116Z","shell.execute_reply.started":"2023-02-23T10:53:21.268682Z","shell.execute_reply":"2023-02-23T10:53:26.250359Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Define the input shape for the model\nmax_length = 98\n\n# Define the custom top layer for classification\nnum_labels = 3\ntop_layer = tf.keras.layers.Dense(num_labels, activation='softmax')\n\n# Define the GPT-2 model with the custom top layer\ninput_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\ngpt2_output = gpt2_model(input_ids)[0]\ngpt2_output = gpt2_output[:, -1, :]\ngpt2_output = top_layer(gpt2_output)\ngpt2_model = tf.keras.models.Model(inputs=input_ids, outputs=gpt2_output)\n\n# Define the loss function and metrics for training\nloss = tf.keras.losses.CategoricalCrossentropy()\nmetrics = [tf.keras.metrics.CategoricalAccuracy()]\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n\n# Compile the GPT-2 model for training\ngpt2_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n\n# Train the GPT-2 model on the sentiment analysis task\ngpt2_model.fit(train_dataset, epochs=3, validation_data=val_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:53:26.252664Z","iopub.execute_input":"2023-02-23T10:53:26.252901Z","iopub.status.idle":"2023-02-23T12:32:13.307218Z","shell.execute_reply.started":"2023-02-23T10:53:26.252871Z","shell.execute_reply":"2023-02-23T12:32:13.305771Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['attention_mask'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n","output_type":"stream"},{"name":"stdout","text":"147/147 [==============================] - 2001s 13s/step - loss: 1.1850 - categorical_accuracy: 0.4800 - val_loss: 0.9770 - val_categorical_accuracy: 0.5338\nEpoch 2/3\n147/147 [==============================] - 1925s 13s/step - loss: 1.0113 - categorical_accuracy: 0.5087 - val_loss: 0.9935 - val_categorical_accuracy: 0.5535\nEpoch 3/3\n147/147 [==============================] - 1954s 13s/step - loss: 0.9545 - categorical_accuracy: 0.5632 - val_loss: 0.8883 - val_categorical_accuracy: 0.6279\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fe06430dfd0>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Conclusion:","metadata":{}},{"cell_type":"markdown","source":"## As we observed that the best method is transfer learning for models **(BERT, RoBERTa)** which have approximately **85% accuracy for train and more than 81% for test** with only **three** epoches.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}