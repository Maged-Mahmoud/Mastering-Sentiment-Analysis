{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/magedmahmoud/mastering-sentiment-analysis?scriptVersionId=91949569\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Frame the problem and look at the big picture:\nIn this notebook we use various techniques and recent methods in text classification from rule based sentiment analysis to Machine learning based Sentiment Analysis.","metadata":{}},{"cell_type":"markdown","source":"**This Notebook will give step by step approach to NLP Sentiment Analysis from basic models to deep learning models** ","metadata":{}},{"cell_type":"markdown","source":"### What is Sentiment Analysis?\nSentiment Analysis is a NLP technique used to interpret the emotions, comments and reviews (Positive, Negative or Natural) behind text data.\n### Applications of Sentiment Analysis:\n* Social Media Analysis\n* Public Sentiment about products\n* Content Moderation\n* Stock Market Analysis\n","metadata":{}},{"cell_type":"markdown","source":"**Some Code here is imported from **SOHAIL notebook****","metadata":{}},{"cell_type":"markdown","source":"# Get The Data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-02T22:19:40.948222Z","iopub.execute_input":"2022-04-02T22:19:40.948552Z","iopub.status.idle":"2022-04-02T22:19:41.009213Z","shell.execute_reply.started":"2022-04-02T22:19:40.94847Z","shell.execute_reply":"2022-04-02T22:19:41.008492Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin.gz\n/kaggle/input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\n/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\n/kaggle/input/glove2word2vec/glove_w2v.txt\n/kaggle/input/financial-sentiment-analysis/data.csv\n/kaggle/input/fast-text-embeddings-without-subwords/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec\n/kaggle/input/fast-text-embeddings-without-subwords/crawl-300d-2M.vec/crawl-300d-2M.vec\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Import Files and Libraries","metadata":{}},{"cell_type":"code","source":"pip install tensorflow_text","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:19:41.010836Z","iopub.execute_input":"2022-04-02T22:19:41.011149Z","iopub.status.idle":"2022-04-02T22:20:48.503332Z","shell.execute_reply.started":"2022-04-02T22:19:41.011112Z","shell.execute_reply":"2022-04-02T22:20:48.502454Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting tensorflow_text\n  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n     |████████████████████████████████| 4.9 MB 893 kB/s            \n\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_text) (0.12.0)\nCollecting tensorflow<2.9,>=2.8.0\n  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\n     |████████████████████████████████| 497.5 MB 21 kB/s              \n\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.43.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (4.0.1)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.6.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (59.5.0)\nRequirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.4.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.1.0)\nRequirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.15.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.2.0)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.20.3)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.13.3)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.19.1)\nCollecting libclang>=9.0.1\n  Downloading libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n     |████████████████████████████████| 14.5 MB 38.1 MB/s            \n\u001b[?25hCollecting keras<2.9,>=2.8.0rc0\n  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n     |████████████████████████████████| 1.4 MB 48.3 MB/s            \n\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.3.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.1.0)\nCollecting tensorboard<2.9,>=2.8\n  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n     |████████████████████████████████| 5.8 MB 47.5 MB/s            \n\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.1.2)\nCollecting tf-estimator-nightly==2.8.0.dev2021122109\n  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n     |████████████████████████████████| 462 kB 50.5 MB/s            \n\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.16.0)\nCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n     |████████████████████████████████| 2.1 MB 47.8 MB/s            \n\u001b[?25hRequirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.12)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.37.0)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.5.2)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.8.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.3.6)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.4.6)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.6.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.35.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2.26.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2.0.2)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.3.0)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.10.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2021.10.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.6.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.1.1)\nInstalling collected packages: tf-estimator-nightly, tensorflow-io-gcs-filesystem, tensorboard, libclang, keras, tensorflow, tensorflow-text\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.6.0\n    Uninstalling tensorboard-2.6.0:\n      Successfully uninstalled tensorboard-2.6.0\n  Attempting uninstall: keras\n    Found existing installation: keras 2.6.0\n    Uninstalling keras-2.6.0:\n      Successfully uninstalled keras-2.6.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.6.2\n    Uninstalling tensorflow-2.6.2:\n      Successfully uninstalled tensorflow-2.6.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nexplainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\ntfx-bsl 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ntfx-bsl 1.5.0 requires numpy<1.20,>=1.16, but you have numpy 1.20.3 which is incompatible.\ntfx-bsl 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\ntensorflow-transform 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ntensorflow-transform 1.5.0 requires numpy<1.20,>=1.16, but you have numpy 1.20.3 which is incompatible.\ntensorflow-transform 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\ntensorflow-transform 1.5.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,<2.8,>=1.15.2, but you have tensorflow 2.8.0 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.8.0 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.24.0 which is incompatible.\u001b[0m\nSuccessfully installed keras-2.8.0 libclang-13.0.0 tensorboard-2.8.0 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 tensorflow-text-2.8.1 tf-estimator-nightly-2.8.0.dev2021122109\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# nltk imports\nfrom nltk.tokenize import word_tokenize # tokenize the text == the text is splitted into words in list\nfrom nltk.corpus import stopwords # this contain common stop words that has no effect in analysis\nfrom nltk.stem import WordNetLemmatizer #Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item\n# sklearn imports\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # bags of words and TF IDF\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix , make_scorer# classification Metrics\nfrom sklearn.naive_bayes import MultinomialNB # Multiclassification\nfrom sklearn.preprocessing import MinMaxScaler \nfrom sklearn import svm\nfrom sklearn.model_selection import StratifiedKFold # For stratified splitting (helpful in imbalanced data)\nfrom sklearn.preprocessing import LabelBinarizer # for Categorical features\nfrom sklearn.model_selection import GridSearchCV # for tuning parameters\nfrom sklearn.model_selection import train_test_split # splitting dataset\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.svm import SVC\nfrom sklearn import metrics\nfrom sklearn import pipeline\nfrom sklearn import linear_model\n############################################################################\nfrom gensim.models import KeyedVectors # to save and load vectors\nimport string\nimport re\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport catboost as cbt\n# tensorflow and keras\nimport keras\nfrom keras.layers.embeddings import Embedding\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import sequence, text\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:20:48.505078Z","iopub.execute_input":"2022-04-02T22:20:48.505335Z","iopub.status.idle":"2022-04-02T22:20:54.085315Z","shell.execute_reply.started":"2022-04-02T22:20:48.505298Z","shell.execute_reply":"2022-04-02T22:20:54.084498Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/financial-sentiment-analysis/data.csv')\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:20:54.087332Z","iopub.execute_input":"2022-04-02T22:20:54.087539Z","iopub.status.idle":"2022-04-02T22:20:54.133377Z","shell.execute_reply.started":"2022-04-02T22:20:54.08751Z","shell.execute_reply":"2022-04-02T22:20:54.132657Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                            Sentence Sentiment\n0  The GeoSolutions technology will leverage Bene...  positive\n1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n2  For the last quarter of 2010 , Componenta 's n...  positive\n3  According to the Finnish-Russian Chamber of Co...   neutral\n4  The Swedish buyout firm has sold its remaining...   neutral","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The GeoSolutions technology will leverage Bene...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>For the last quarter of 2010 , Componenta 's n...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>According to the Finnish-Russian Chamber of Co...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The Swedish buyout firm has sold its remaining...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:20:54.134705Z","iopub.execute_input":"2022-04-02T22:20:54.134963Z","iopub.status.idle":"2022-04-02T22:20:54.139875Z","shell.execute_reply.started":"2022-04-02T22:20:54.134928Z","shell.execute_reply":"2022-04-02T22:20:54.139192Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(5842, 2)"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:20:54.141454Z","iopub.execute_input":"2022-04-02T22:20:54.141882Z","iopub.status.idle":"2022-04-02T22:20:54.169176Z","shell.execute_reply.started":"2022-04-02T22:20:54.141845Z","shell.execute_reply":"2022-04-02T22:20:54.168481Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5842 entries, 0 to 5841\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   Sentence   5842 non-null   object\n 1   Sentiment  5842 non-null   object\ndtypes: object(2)\nmemory usage: 91.4+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"unique_sentiments = df.Sentiment.unique()\nunique_sentiments","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:20:54.170468Z","iopub.execute_input":"2022-04-02T22:20:54.170725Z","iopub.status.idle":"2022-04-02T22:20:54.179524Z","shell.execute_reply.started":"2022-04-02T22:20:54.170689Z","shell.execute_reply":"2022-04-02T22:20:54.178661Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array(['positive', 'negative', 'neutral'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"df.Sentiment.value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:20:54.180803Z","iopub.execute_input":"2022-04-02T22:20:54.181063Z","iopub.status.idle":"2022-04-02T22:20:54.397389Z","shell.execute_reply.started":"2022-04-02T22:20:54.181028Z","shell.execute_reply":"2022-04-02T22:20:54.396618Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# mapping to the sentiment column \n\ndicto = {'positive': 1, 'neutral': 0 , 'negative': -1}\n\ndf.Sentiment = df.Sentiment.map(dicto)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:20:54.398684Z","iopub.execute_input":"2022-04-02T22:20:54.399208Z","iopub.status.idle":"2022-04-02T22:20:54.406057Z","shell.execute_reply.started":"2022-04-02T22:20:54.399164Z","shell.execute_reply":"2022-04-02T22:20:54.404814Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Classical models function\ndef classical_model(df = df,bow = False,TFIDF = False,Ngram = False, model = linear_model.LogisticRegression(solver = 'liblinear')): \n    '''\n    this function automate the classical models to train and evaluate Sentiment Analysis Models\n    firstly: you must specify one type of text vectorization (bow, TFIDF, Ngram).\n    bow = True or False          # bag of words\n    TFIDF = True or False\n    Ngram = shape of Ngram_range (...,...)\n    model should be specified also to define which model will be trained\n    '''\n    df['kfold'] = -1\n    \n    df = df.sample(frac=1).reset_index(drop= True) # this shuffle data and reset index\n    \n    y = df.Sentiment.values\n    \n    # Intitiate kfold class from model_selection module\n    \n    kf = StratifiedKFold(n_splits = 5)\n    \n    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n        df.loc[v_, 'kfold'] = f\n    \n    if bow:\n        count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern=None)\n    if TFIDF:\n        count_vec = TfidfVectorizer(tokenizer = word_tokenize, token_pattern=None)\n    if Ngram:\n        count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern=None, ngram_range = Ngram)\n    # we go over the folds\n    for fold_ in range(5):\n        train_df = df[df.kfold != fold_].reset_index(drop=True)\n        test_df = df[df.kfold == fold_].reset_index(drop=True)\n        count_vec.fit(train_df.Sentence)\n        \n        xtrain = count_vec.transform(train_df.Sentence)\n        xtest = count_vec.transform(test_df.Sentence)\n        \n        model.fit(xtrain,train_df.Sentiment)\n        \n        preds = model.predict(xtest)\n        \n        accuracy_precision = precision_score(test_df.Sentiment, preds, average='micro')\n        accuracy_recall = recall_score(test_df.Sentiment, preds, average = 'micro')\n        \n        print('precision score:', accuracy_precision)\n        print('recall score:', accuracy_recall)\n        print(\"========================================================\")\n        \n    print(classification_report(test_df.Sentiment, preds))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:20:54.409788Z","iopub.execute_input":"2022-04-02T22:20:54.410169Z","iopub.status.idle":"2022-04-02T22:20:54.422358Z","shell.execute_reply.started":"2022-04-02T22:20:54.41013Z","shell.execute_reply":"2022-04-02T22:20:54.421566Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Model-1 Logistic + BOW","metadata":{}},{"cell_type":"code","source":"#Baseline model let's start with a logistic regression model since it is the fastest for high dimensional sparse data\n\nclassical_model(df, bow =True,model=linear_model.LogisticRegression(solver = 'liblinear'))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:20:54.423691Z","iopub.execute_input":"2022-04-02T22:20:54.424055Z","iopub.status.idle":"2022-04-02T22:21:10.822516Z","shell.execute_reply.started":"2022-04-02T22:20:54.424018Z","shell.execute_reply":"2022-04-02T22:21:10.821732Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"precision score: 0.6766467065868264\nrecall score: 0.6766467065868264\n========================================================\nprecision score: 0.6911890504704876\nrecall score: 0.6911890504704876\n========================================================\nprecision score: 0.6986301369863014\nrecall score: 0.6986301369863014\n========================================================\nprecision score: 0.6892123287671232\nrecall score: 0.6892123287671232\n========================================================\nprecision score: 0.6797945205479452\nrecall score: 0.6797945205479452\n========================================================\n              precision    recall  f1-score   support\n\n          -1       0.26      0.22      0.24       172\n           0       0.71      0.78      0.74       626\n           1       0.80      0.72      0.76       370\n\n    accuracy                           0.68      1168\n   macro avg       0.59      0.57      0.58      1168\nweighted avg       0.67      0.68      0.67      1168\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see that labels **1,0 have more than 70 percent** score but label **-1 score is pretty low**, hence we can conclude that we need to go on modelling untill these scores get better.","metadata":{}},{"cell_type":"markdown","source":"# Model 2 NaiveBayes + BOW","metadata":{}},{"cell_type":"code","source":"# Lets try with NaiveBayes model \n\nclassical_model(bow = True,model = MultinomialNB())","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:21:10.827129Z","iopub.execute_input":"2022-04-02T22:21:10.829382Z","iopub.status.idle":"2022-04-02T22:21:24.764487Z","shell.execute_reply.started":"2022-04-02T22:21:10.829335Z","shell.execute_reply":"2022-04-02T22:21:24.763803Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"precision score: 0.6946107784431138\nrecall score: 0.6946107784431138\n========================================================\nprecision score: 0.6963216424294268\nrecall score: 0.6963216424294268\n========================================================\nprecision score: 0.699486301369863\nrecall score: 0.699486301369863\n========================================================\nprecision score: 0.6806506849315068\nrecall score: 0.6806506849315068\n========================================================\nprecision score: 0.7148972602739726\nrecall score: 0.7148972602739726\n========================================================\n              precision    recall  f1-score   support\n\n          -1       0.49      0.32      0.39       172\n           0       0.73      0.86      0.79       626\n           1       0.75      0.65      0.69       370\n\n    accuracy                           0.71      1168\n   macro avg       0.66      0.61      0.63      1168\nweighted avg       0.70      0.71      0.70      1168\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It is evident that the for **label -1 the metrics values little improved** so this model will be the new baseline model for this problem.","metadata":{}},{"cell_type":"markdown","source":"# Model-3 Naive Bayes with TFIDF","metadata":{}},{"cell_type":"code","source":"# Now lets try with TF-IDF vectorizer instead of bag of words to MultinomialNB().\n\nclassical_model(model= MultinomialNB(),TFIDF=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:21:24.768063Z","iopub.execute_input":"2022-04-02T22:21:24.775549Z","iopub.status.idle":"2022-04-02T22:21:38.741032Z","shell.execute_reply.started":"2022-04-02T22:21:24.77551Z","shell.execute_reply":"2022-04-02T22:21:38.740295Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"precision score: 0.6467065868263473\nrecall score: 0.6467065868263473\n========================================================\nprecision score: 0.6390076988879384\nrecall score: 0.6390076988879384\n========================================================\nprecision score: 0.6541095890410958\nrecall score: 0.6541095890410958\n========================================================\nprecision score: 0.6618150684931506\nrecall score: 0.6618150684931506\n========================================================\nprecision score: 0.6472602739726028\nrecall score: 0.6472602739726028\n========================================================\n              precision    recall  f1-score   support\n\n          -1       1.00      0.03      0.06       172\n           0       0.63      0.98      0.77       626\n           1       0.72      0.37      0.49       370\n\n    accuracy                           0.65      1168\n   macro avg       0.78      0.46      0.44      1168\nweighted avg       0.71      0.65      0.57      1168\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"also, the **label -1 metrics recall score is unsatisfied**  ","metadata":{}},{"cell_type":"markdown","source":"# Model-4 Naive Bayes with Ngrams","metadata":{}},{"cell_type":"code","source":"# Now to the baseline bag of word Naivebayes model lets apply Ngrams  and compare the results.\n\n# Lets try with NaiveBayes model \n\nclassical_model(model = MultinomialNB(), Ngram=(1,3))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:21:38.742135Z","iopub.execute_input":"2022-04-02T22:21:38.742541Z","iopub.status.idle":"2022-04-02T22:21:57.25967Z","shell.execute_reply.started":"2022-04-02T22:21:38.742502Z","shell.execute_reply":"2022-04-02T22:21:57.258935Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"precision score: 0.6467065868263473\nrecall score: 0.6467065868263473\n========================================================\nprecision score: 0.6449957228400343\nrecall score: 0.6449957228400343\n========================================================\nprecision score: 0.6626712328767124\nrecall score: 0.6626712328767124\n========================================================\nprecision score: 0.6883561643835616\nrecall score: 0.6883561643835616\n========================================================\nprecision score: 0.651541095890411\nrecall score: 0.651541095890411\n========================================================\n              precision    recall  f1-score   support\n\n          -1       0.22      0.14      0.17       172\n           0       0.68      0.82      0.74       626\n           1       0.74      0.60      0.66       370\n\n    accuracy                           0.65      1168\n   macro avg       0.55      0.52      0.53      1168\nweighted avg       0.63      0.65      0.63      1168\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"So the **ngrams has even decreased the metrics** value.Hence it is evident that the baseline model is still the best one till now","metadata":{}},{"cell_type":"markdown","source":"# Text processing","metadata":{}},{"cell_type":"markdown","source":"I will **not remove stopwords** in this because it might change the context of the sentence.\n\ne.g **\"He is not a good person\"** will be changed into **\" 'He' , 'good', 'person'\"** which is the complete opposite of the sentence.","metadata":{}},{"cell_type":"code","source":"# Now lets do some cleaning on the text data and apply it to baseline model and compare the accuracies.\n\n\n\ndef process_text(text):\n    \n    text = word_tokenize(text) # tokenize words in text\n    text = [re.sub('[^A-Za-z]+', '', word) for word in text] # this line substitutes any white space before the word by removing the space\n    text = [word.lower() for word in text if word.isalpha()] # lower each word in text\n    text = [WordNetLemmatizer().lemmatize(word) for word in text] # lemmatization of words, so when see persons an person, both are dealt as one word person\n    text = ' '.join(text) # join words into text again\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:21:57.261053Z","iopub.execute_input":"2022-04-02T22:21:57.261461Z","iopub.status.idle":"2022-04-02T22:21:57.268225Z","shell.execute_reply.started":"2022-04-02T22:21:57.261423Z","shell.execute_reply":"2022-04-02T22:21:57.267481Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"text = 'He is not a good person'\ntoken_text = word_tokenize(text)\n[ word for word in token_text if word not in stopwords.words('english')]","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:21:57.27109Z","iopub.execute_input":"2022-04-02T22:21:57.271286Z","iopub.status.idle":"2022-04-02T22:21:57.286358Z","shell.execute_reply.started":"2022-04-02T22:21:57.27126Z","shell.execute_reply":"2022-04-02T22:21:57.285687Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['He', 'good', 'person']"},"metadata":{}}]},{"cell_type":"code","source":"df.Sentence = df.Sentence.apply(process_text) # this line applies process_text function to Sentence in dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:21:57.287599Z","iopub.execute_input":"2022-04-02T22:21:57.288254Z","iopub.status.idle":"2022-04-02T22:22:01.690342Z","shell.execute_reply.started":"2022-04-02T22:21:57.288217Z","shell.execute_reply":"2022-04-02T22:22:01.689604Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Model 5 cleaned text Naive Bayes BOW and ","metadata":{}},{"cell_type":"code","source":"#Now lets try this on our baseline MultinomialNB bagofwords model\n\nclassical_model(model = MultinomialNB(),bow = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:22:01.691425Z","iopub.execute_input":"2022-04-02T22:22:01.691673Z","iopub.status.idle":"2022-04-02T22:22:13.145986Z","shell.execute_reply.started":"2022-04-02T22:22:01.691641Z","shell.execute_reply":"2022-04-02T22:22:13.145214Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"precision score: 0.7048759623609923\nrecall score: 0.7048759623609923\n========================================================\nprecision score: 0.7159965782720273\nrecall score: 0.7159965782720273\n========================================================\nprecision score: 0.7071917808219178\nrecall score: 0.7071917808219178\n========================================================\nprecision score: 0.7003424657534246\nrecall score: 0.7003424657534246\n========================================================\nprecision score: 0.6943493150684932\nrecall score: 0.6943493150684932\n========================================================\n              precision    recall  f1-score   support\n\n          -1       0.43      0.33      0.38       172\n           0       0.73      0.84      0.78       626\n           1       0.73      0.62      0.67       370\n\n    accuracy                           0.69      1168\n   macro avg       0.63      0.60      0.61      1168\nweighted avg       0.68      0.69      0.68      1168\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"after some text cleaning **we beat our baseline model by small margin** hence the above model will be our new baseline model.","metadata":{}},{"cell_type":"markdown","source":"Now lets go into **word embeddings** \n\nIn the above models each word token is converted into integer tokens by (BOW and TFIDF) now lets convert these **integer tokens into vectors.**","metadata":{}},{"cell_type":"markdown","source":"# Model 6 Fastext vector with Naive Bayes Baseline ","metadata":{}},{"cell_type":"code","source":"# In this model we will use fastText vectors and also convert each word vector in sentence vector.\n\n#The code is taken from https://fasttext.cc/docs/en/english-vectors.html, this code splits each vector by \n# space and return for more info go through the above linl\n \ndef sentence_to_vec(s, embedding_dict, tokenizer):\n    \n    words = tokenizer(s)\n    \n    embedding_list = []\n    for w in words:\n        \n        if w in embedding_dict:\n            embedding_list.append(embedding_dict[w])\n            \n    # if we dont have any vectors, then return zeros\n    if len(embedding_list) == 0:\n        return np.zeros(300)\n    \n    #convert list of embeddings into an array\n    embedding_list = np.array(embedding_list)\n    #calculate sum over axis = 0 (This will convert the word vectorrs to sentence vectors)\n    v = embedding_list.sum(axis=0)\n    \n    #return normalized vector\n    return v\n\ndf = df.sample(frac=1).reset_index(drop=True)\ny = df.Sentiment.values\n\ndf.drop('kfold', axis=1, inplace = True)\n\nkf = StratifiedKFold(n_splits = 5)\n\n#load embeddings into memory\nprint(\"Loading embeddings\")\nembeddings = KeyedVectors.load_word2vec_format('/kaggle/input/fast-text-embeddings-without-subwords/crawl-300d-2M.vec/crawl-300d-2M.vec')\n    \n#create sentence embeddings\nprint(\"creating sentence vectors\")\nvectors = []\nfor sentence in df.Sentence.values:\n    vectors.append(         \n       sentence_to_vec(s = sentence, embedding_dict= embeddings, tokenizer = word_tokenize)\n    )\nvectors = np.array(vectors)\n    \nfor fold_,(train_, valid_) in enumerate(kf.split(X=df, y=y)):\n        print(\"fold: \", fold_)\n        \n        xtrain = vectors[train_,:]\n        ytrain = y[train_]\n        \n        xtest = vectors[valid_,:]\n        ytest = y[valid_]\n        \n        scaler = MinMaxScaler()\n        \n        xscaled_train = scaler.fit_transform(xtrain)\n        xscaled_test = scaler.transform(xtest)\n        \n        model = MultinomialNB()\n    \n        model.fit(xscaled_train,ytrain)\n    \n        y_pred = model.predict(xscaled_test)\n    \n        pres_score = precision_score(ytest, y_pred, average='micro')\n    \n        rec_score = recall_score(ytest, y_pred, average='micro')\n    \n        print('precision and recall scores:', pres_score, rec_score)\n        print(\"======================================================\")\n    \nprint(classification_report(ytest, y_pred, labels=[1,-1,0]))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:22:13.147206Z","iopub.execute_input":"2022-04-02T22:22:13.148917Z","iopub.status.idle":"2022-04-02T22:29:37.049254Z","shell.execute_reply.started":"2022-04-02T22:22:13.148875Z","shell.execute_reply":"2022-04-02T22:29:37.046615Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Loading embeddings\ncreating sentence vectors\nfold:  0\nprecision and recall scores: 0.5372112917023096 0.5372112917023096\n======================================================\nfold:  1\nprecision and recall scores: 0.5380667236954663 0.5380667236954663\n======================================================\nfold:  2\nprecision and recall scores: 0.5376712328767124 0.5376712328767124\n======================================================\nfold:  3\nprecision and recall scores: 0.5376712328767124 0.5376712328767124\n======================================================\nfold:  4\nprecision and recall scores: 0.535958904109589 0.535958904109589\n======================================================\n              precision    recall  f1-score   support\n\n           1       0.50      0.00      0.01       370\n          -1       0.00      0.00      0.00       172\n           0       0.54      1.00      0.70       626\n\n    accuracy                           0.54      1168\n   macro avg       0.35      0.33      0.23      1168\nweighted avg       0.45      0.54      0.38      1168\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Whoa! It is our worst performing model so far! What should we do now? Should we ask muppets to save us? No! we will keep on trying with other models and methods.Thats what data scientists do!**","metadata":{}},{"cell_type":"markdown","source":"# Model 7 - Glove vectors with Baseline model","metadata":{}},{"cell_type":"code","source":"class Word2VecVectorizer:\n    def __init__(self, model):\n        print(\"Loading in word vectors...\")\n        self.word_vectors = model\n        print(\"Finished loading in word vectors\")\n    \n    def fit(self, data):\n        pass\n    \n    def transform(self, data):\n        # determine the dimensionality of vectors\n        v = self.word_vectors.get_vector('king')\n        self.D = v.shape[0]\n\n        X = np.zeros((len(data), self.D))\n        n = 0\n        emptycount = 0\n    \n        for sentence in data:\n            tokens = sentence.split()\n            vecs = []\n            m = 0\n            for word in tokens:\n                try:\n                    vec = self.word_vectors.get_vector(word)\n                    vecs.append(vec)\n                    m += 1\n                except KeyError:\n                    pass\n            if len(vecs) > 0:\n                vecs = np.array(vecs)\n                X[n] = vecs.mean(axis=0)\n            else:\n                emptycount += 1\n            n += 1\n        print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n        return X\n\n\n    def fit_transform(self, data):\n        self.fit(data)\n        return self.transform(data)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:29:37.050435Z","iopub.execute_input":"2022-04-02T22:29:37.050862Z","iopub.status.idle":"2022-04-02T22:29:37.06847Z","shell.execute_reply.started":"2022-04-02T22:29:37.050824Z","shell.execute_reply":"2022-04-02T22:29:37.067645Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\nword2vec_output_file = '/kaggle/input/glove2word2vec/glove_w2v.txt'\n\nmodel = KeyedVectors.load_word2vec_format(word2vec_output_file, binary = False)\n\nXtrain,Xtest,ytrain,ytest = train_test_split(df.Sentence, df.Sentiment, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:29:37.069756Z","iopub.execute_input":"2022-04-02T22:29:37.070201Z","iopub.status.idle":"2022-04-02T22:30:38.093727Z","shell.execute_reply.started":"2022-04-02T22:29:37.070138Z","shell.execute_reply":"2022-04-02T22:30:38.092991Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#we create a Vectorizer object that will help us to transform our reviews to vectors, a numerical representation. \n#Then we can use those vectors to feed our classifier.\n\nvectorizer = Word2VecVectorizer(model)\n\nX_train = vectorizer.fit_transform(Xtrain)\ny_train = ytrain\n\nX_test = vectorizer.transform(Xtest)\ny_test = ytest","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:30:38.094964Z","iopub.execute_input":"2022-04-02T22:30:38.095206Z","iopub.status.idle":"2022-04-02T22:30:38.419035Z","shell.execute_reply.started":"2022-04-02T22:30:38.095173Z","shell.execute_reply":"2022-04-02T22:30:38.418181Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Loading in word vectors...\nFinished loading in word vectors\nNumer of samples with no words found: 0 / 4673\nNumer of samples with no words found: 0 / 1169\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the parameters to tune\nparameters = { \n    'C': [1.0, 10],\n    'gamma': [1, 'auto', 'scale']\n}\n\nmodel = GridSearchCV(SVC(kernel='rbf'), parameters, cv=5, n_jobs=-1)\n\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:30:38.42036Z","iopub.execute_input":"2022-04-02T22:30:38.420711Z","iopub.status.idle":"2022-04-02T22:32:12.586549Z","shell.execute_reply.started":"2022-04-02T22:30:38.42067Z","shell.execute_reply":"2022-04-02T22:32:12.584716Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n          -1       0.45      0.27      0.34       172\n           0       0.72      0.85      0.78       628\n           1       0.74      0.66      0.70       369\n\n    accuracy                           0.70      1169\n   macro avg       0.64      0.59      0.61      1169\nweighted avg       0.69      0.70      0.69      1169\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Model 7 is our best performing model,then model 5, lets do some hyperparameter tuning before we move on to deep learning models**","metadata":{}},{"cell_type":"markdown","source":"# Model # Hyperparameter tuning best models","metadata":{}},{"cell_type":"code","source":"# model 5 Tuning\ndef scoring(y_train,y_pred):\n    return f1_score(y_train,y_pred,average='weighted')\n\ncount_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern = None)\n\nX_train = count_vec.fit_transform(df.Sentence)\n\nparameters = {'alpha': [0.001,0.01,0.1,0.2,0.3,0.5,0.7,1,1.5,1.6,1.8,10,100]}\n\nmodel = MultinomialNB()\n\ngrid_search = GridSearchCV(model , parameters, cv=5, scoring = make_scorer(scoring), n_jobs = -1)\n\ngrid_result = grid_search.fit(X_train, df.Sentiment)\n\nprint('Best params: ', grid_result.best_params_)\nprint('Best score: ', grid_result.best_score_)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:32:12.589139Z","iopub.execute_input":"2022-04-02T22:32:12.589577Z","iopub.status.idle":"2022-04-02T22:32:14.267933Z","shell.execute_reply.started":"2022-04-02T22:32:12.589534Z","shell.execute_reply":"2022-04-02T22:32:14.266258Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Best params:  {'alpha': 1}\nBest score:  0.6979798529675103\n","output_type":"stream"}]},{"cell_type":"code","source":"# Lets also check svm hyperparameters using pipelines\n\ndf = df.sample(frac=1).reset_index(drop=True)\n\n\n\ncount_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern = None)\n\nX_train = count_vec.fit_transform(df.Sentence)\n\n# defining parameter range\nparam_grid = {'C': [0.1, 1, 10, 100],\n              'gamma': [1, 0.1, 0.01, 0.001],\n              'kernel': ['rbf','linear']}\n \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 1, cv=5, scoring = make_scorer(scoring), n_jobs = -1)\n \n# fitting the model for grid search\ngrid.fit(X_train, df.Sentiment)\n\nprint('Best params: ', grid.best_params_)\nprint('Best score: ', grid.best_score_)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:32:14.269343Z","iopub.execute_input":"2022-04-02T22:32:14.269599Z","iopub.status.idle":"2022-04-02T22:41:06.442033Z","shell.execute_reply.started":"2022-04-02T22:32:14.269563Z","shell.execute_reply":"2022-04-02T22:41:06.441209Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 32 candidates, totalling 160 fits\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.4min\n[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:  8.8min finished\n","output_type":"stream"},{"name":"stdout","text":"Best params:  {'C': 0.1, 'gamma': 1, 'kernel': 'linear'}\nBest score:  0.6924175105868348\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**The model has performed worser than the previous above model, we can also create different pipelines of different machine learning models and compare them with both BOW and tfidf models**\n\n**More models including deep learning models are yet to come**\n","metadata":{}},{"cell_type":"markdown","source":"# Model- 8 LSTM using glove word2vec","metadata":{}},{"cell_type":"code","source":"# loading pretrained google news word2vec embedding 300D\nfrom gensim.models import KeyedVectors\nword2vec_pretrained = KeyedVectors.load_word2vec_format(\"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\",binary=True)\nword2vec_pretrained_dict = dict(zip(word2vec_pretrained.key_to_index.keys(), \n                                    word2vec_pretrained.vectors))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:41:06.443365Z","iopub.execute_input":"2022-04-02T22:41:06.443623Z","iopub.status.idle":"2022-04-02T22:42:11.103023Z","shell.execute_reply.started":"2022-04-02T22:41:06.443587Z","shell.execute_reply":"2022-04-02T22:42:11.102234Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"list(word2vec_pretrained_dict.values())[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:42:11.104191Z","iopub.execute_input":"2022-04-02T22:42:11.106129Z","iopub.status.idle":"2022-04-02T22:42:11.194728Z","shell.execute_reply.started":"2022-04-02T22:42:11.106097Z","shell.execute_reply":"2022-04-02T22:42:11.193818Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(300,)"},"metadata":{}}]},{"cell_type":"code","source":"df['Sentence'] = df['Sentence'].apply(process_text)\n\nX_train,X_test,y_train,y_test = train_test_split(df.Sentence, df.Sentiment, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:42:11.200141Z","iopub.execute_input":"2022-04-02T22:42:11.200483Z","iopub.status.idle":"2022-04-02T22:42:13.674183Z","shell.execute_reply.started":"2022-04-02T22:42:11.200452Z","shell.execute_reply":"2022-04-02T22:42:13.673277Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"y_train_enc = np_utils.to_categorical(y_train, 3)\ny_test_enc = np_utils.to_categorical(y_test, 3)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:42:13.67582Z","iopub.execute_input":"2022-04-02T22:42:13.676057Z","iopub.status.idle":"2022-04-02T22:42:13.680715Z","shell.execute_reply.started":"2022-04-02T22:42:13.676024Z","shell.execute_reply":"2022-04-02T22:42:13.68007Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"y_train_enc","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:42:13.682088Z","iopub.execute_input":"2022-04-02T22:42:13.682536Z","iopub.status.idle":"2022-04-02T22:42:13.694788Z","shell.execute_reply.started":"2022-04-02T22:42:13.682502Z","shell.execute_reply":"2022-04-02T22:42:13.694175Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"array([[0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       ...,\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"token = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n\ntoken.fit_on_texts(list(X_train) + list(X_test)) # fits tokens on texts\nxtrain_seq = token.texts_to_sequences(X_train) # text to sequences converts the sentence words to number sequences\nxtest_seq = token.texts_to_sequences(X_test)\n\n#zero pad sequences \n\nxtrain_pad = sequence.pad_sequences(xtrain_seq) # zero padding all sentences to have the same shape as the largest one\nxtest_pad = sequence.pad_sequences(xtest_seq)\n\nword_index = token.word_index # returns the word index that have been tokenized","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:42:13.696087Z","iopub.execute_input":"2022-04-02T22:42:13.696524Z","iopub.status.idle":"2022-04-02T22:42:14.123716Z","shell.execute_reply.started":"2022-04-02T22:42:13.696489Z","shell.execute_reply":"2022-04-02T22:42:14.122988Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#create embedding matrix for words that we have in dataset\n\nembedding_matrix = np.zeros((len(word_index)+1, 300))\nfor word,i in word_index.items():\n    embedding_vector = word2vec_pretrained_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:42:14.12492Z","iopub.execute_input":"2022-04-02T22:42:14.125178Z","iopub.status.idle":"2022-04-02T22:42:14.159815Z","shell.execute_reply.started":"2022-04-02T22:42:14.125143Z","shell.execute_reply":"2022-04-02T22:42:14.159054Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Build Custom Metrics (F1-Score)\nfrom keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m_():\n    def f1_m(y_true, y_pred):\n        precision = precision_m(y_true, y_pred)\n        recall = recall_m(y_true, y_pred)\n        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n    return f1_m\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:42:14.160979Z","iopub.execute_input":"2022-04-02T22:42:14.161409Z","iopub.status.idle":"2022-04-02T22:42:14.170381Z","shell.execute_reply.started":"2022-04-02T22:42:14.161367Z","shell.execute_reply":"2022-04-02T22:42:14.169434Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# A simple LSTM with two dense layers\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index)+1,300,weights=[embedding_matrix], trainable = False))\n\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3))\n\nmodel.add(Dense(1024 , activation = 'relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation = 'relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = [f1_m_()])","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:42:14.171836Z","iopub.execute_input":"2022-04-02T22:42:14.172518Z","iopub.status.idle":"2022-04-02T22:42:14.711318Z","shell.execute_reply.started":"2022-04-02T22:42:14.172421Z","shell.execute_reply":"2022-04-02T22:42:14.710585Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"2022-04-02 22:42:14.360990: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n2022-04-02 22:42:14.382937: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\nSkipping registering GPU devices...\n","output_type":"stream"}]},{"cell_type":"code","source":"model.fit(xtrain_pad, y=y_train_enc, batch_size = 512, epochs =10, verbose=1, validation_data = (xtest_pad, y_test_enc))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:42:14.712381Z","iopub.execute_input":"2022-04-02T22:42:14.712616Z","iopub.status.idle":"2022-04-02T22:45:52.91823Z","shell.execute_reply.started":"2022-04-02T22:42:14.712583Z","shell.execute_reply":"2022-04-02T22:45:52.917543Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"2022-04-02 22:42:17.863820: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n2022-04-02 22:42:17.890000: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n2022-04-02 22:42:19.496428: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n2022-04-02 22:42:19.600719: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":" 1/10 [==>...........................] - ETA: 52s - loss: 1.1039 - f1_m: 0.0000e+00","output_type":"stream"},{"name":"stderr","text":"2022-04-02 22:42:20.659196: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"10/10 [==============================] - 25s 2s/step - loss: 1.0028 - f1_m: 0.3547 - val_loss: 0.9403 - val_f1_m: 0.4781\nEpoch 2/10\n10/10 [==============================] - 22s 2s/step - loss: 0.9265 - f1_m: 0.4850 - val_loss: 0.9022 - val_f1_m: 0.4038\nEpoch 3/10\n10/10 [==============================] - 21s 2s/step - loss: 0.8864 - f1_m: 0.4978 - val_loss: 0.8308 - val_f1_m: 0.5058\nEpoch 4/10\n10/10 [==============================] - 21s 2s/step - loss: 0.8460 - f1_m: 0.5345 - val_loss: 0.7915 - val_f1_m: 0.5673\nEpoch 5/10\n10/10 [==============================] - 22s 2s/step - loss: 0.8239 - f1_m: 0.5635 - val_loss: 0.7806 - val_f1_m: 0.5778\nEpoch 6/10\n10/10 [==============================] - 22s 2s/step - loss: 0.8080 - f1_m: 0.5775 - val_loss: 0.7849 - val_f1_m: 0.5480\nEpoch 7/10\n10/10 [==============================] - 21s 2s/step - loss: 0.7819 - f1_m: 0.6036 - val_loss: 0.7319 - val_f1_m: 0.6253\nEpoch 8/10\n10/10 [==============================] - 22s 2s/step - loss: 0.7678 - f1_m: 0.6273 - val_loss: 0.7259 - val_f1_m: 0.6378\nEpoch 9/10\n10/10 [==============================] - 22s 2s/step - loss: 0.7537 - f1_m: 0.6091 - val_loss: 0.7117 - val_f1_m: 0.6311\nEpoch 10/10\n10/10 [==============================] - 21s 2s/step - loss: 0.7399 - f1_m: 0.6398 - val_loss: 0.7096 - val_f1_m: 0.6388\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fa61de89610>"},"metadata":{}}]},{"cell_type":"markdown","source":"**A problem with training neural networks is in the choice of the number of training epochs to use.Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset.**\n\n","metadata":{}},{"cell_type":"markdown","source":"# Model 9 LSTM word2vec + early stopping","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics = [f1_m_()])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\nhistory = model.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:45:52.91949Z","iopub.execute_input":"2022-04-02T22:45:52.920274Z","iopub.status.idle":"2022-04-02T22:50:57.001372Z","shell.execute_reply.started":"2022-04-02T22:45:52.920235Z","shell.execute_reply":"2022-04-02T22:50:57.000597Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Epoch 1/100\n10/10 [==============================] - 26s 2s/step - loss: 0.9945 - f1_m: 0.3665 - val_loss: 0.9296 - val_f1_m: 0.4631\nEpoch 2/100\n10/10 [==============================] - 22s 2s/step - loss: 0.9215 - f1_m: 0.4857 - val_loss: 0.8610 - val_f1_m: 0.5269\nEpoch 3/100\n10/10 [==============================] - 21s 2s/step - loss: 0.8765 - f1_m: 0.5033 - val_loss: 0.8528 - val_f1_m: 0.4490\nEpoch 4/100\n10/10 [==============================] - 21s 2s/step - loss: 0.8531 - f1_m: 0.5291 - val_loss: 0.8012 - val_f1_m: 0.5591\nEpoch 5/100\n10/10 [==============================] - 21s 2s/step - loss: 0.8321 - f1_m: 0.5556 - val_loss: 0.7529 - val_f1_m: 0.6115\nEpoch 6/100\n10/10 [==============================] - 21s 2s/step - loss: 0.8141 - f1_m: 0.5781 - val_loss: 0.7159 - val_f1_m: 0.6437\nEpoch 7/100\n10/10 [==============================] - 23s 2s/step - loss: 0.7862 - f1_m: 0.5859 - val_loss: 0.7542 - val_f1_m: 0.5960\nEpoch 8/100\n10/10 [==============================] - 21s 2s/step - loss: 0.7687 - f1_m: 0.6263 - val_loss: 0.6939 - val_f1_m: 0.6651\nEpoch 9/100\n10/10 [==============================] - 21s 2s/step - loss: 0.7449 - f1_m: 0.6281 - val_loss: 0.6843 - val_f1_m: 0.6540\nEpoch 10/100\n10/10 [==============================] - 22s 2s/step - loss: 0.7387 - f1_m: 0.6339 - val_loss: 0.6882 - val_f1_m: 0.6529\nEpoch 11/100\n10/10 [==============================] - 22s 2s/step - loss: 0.7172 - f1_m: 0.6527 - val_loss: 0.6691 - val_f1_m: 0.6810\nEpoch 12/100\n10/10 [==============================] - 21s 2s/step - loss: 0.7274 - f1_m: 0.6387 - val_loss: 0.6792 - val_f1_m: 0.6583\nEpoch 13/100\n10/10 [==============================] - 21s 2s/step - loss: 0.7158 - f1_m: 0.6437 - val_loss: 0.6810 - val_f1_m: 0.6858\nEpoch 14/100\n10/10 [==============================] - 21s 2s/step - loss: 0.7240 - f1_m: 0.6531 - val_loss: 0.6788 - val_f1_m: 0.6744\nEpoch 14: early stopping\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**F1_Score is close to 73 now, so there is an improvment in precision score. Lets see the classification report**","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(xtest_pad)\n\n# Here '2' is '-1' in previous reports\n\nprint(classification_report(np.argmax(y_test_enc,axis=1), np.argmax(y_pred,axis=1), labels=[0,1,2]))   ","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:50:57.002764Z","iopub.execute_input":"2022-04-02T22:50:57.004158Z","iopub.status.idle":"2022-04-02T22:51:21.810834Z","shell.execute_reply.started":"2022-04-02T22:50:57.004121Z","shell.execute_reply":"2022-04-02T22:51:21.81008Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.76      0.84      0.80       626\n           1       0.77      0.63      0.69       373\n           2       0.45      0.48      0.47       170\n\n    accuracy                           0.72      1169\n   macro avg       0.66      0.65      0.65      1169\nweighted avg       0.72      0.72      0.72      1169\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Its evident that there is a improvement in all the clasification classes, hence this will be our new baseline model.** ","metadata":{}},{"cell_type":"markdown","source":"# Model 10 bidirectional LSTM ","metadata":{}},{"cell_type":"code","source":"model_bi = Sequential()\nmodel_bi.add(Embedding(len(word_index)+1, 300, weights=[embedding_matrix], trainable = False))\n\nmodel_bi.add(SpatialDropout1D(0.3))\nmodel_bi.add(Bidirectional(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3)))\n\nmodel_bi.add(Dense(1024, activation = 'relu'))\nmodel_bi.add(Dropout(0.8))\n\nmodel_bi.add(Dense(1024, activation = 'relu'))\nmodel_bi.add(Dropout(0.8))\n\nmodel_bi.add(Dense(3))\nmodel_bi.add(Activation('softmax'))\nmodel_bi.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = f1_m_())\n\nearlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3, verbose = 0, mode = 'auto')\nhistory = model_bi.fit(xtrain_pad, y=y_train_enc, batch_size = 512, epochs = 100, verbose=1, validation_data = (xtest_pad, y_test_enc),callbacks = [earlystop])\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T22:51:21.812176Z","iopub.execute_input":"2022-04-02T22:51:21.812576Z","iopub.status.idle":"2022-04-02T23:11:12.767694Z","shell.execute_reply.started":"2022-04-02T22:51:21.812536Z","shell.execute_reply":"2022-04-02T23:11:12.766916Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Epoch 1/100\n10/10 [==============================] - 50s 4s/step - loss: 1.0034 - f1_m: 0.3547 - val_loss: 0.9262 - val_f1_m: 0.5169\nEpoch 2/100\n10/10 [==============================] - 42s 4s/step - loss: 0.9213 - f1_m: 0.4999 - val_loss: 0.8778 - val_f1_m: 0.4796\nEpoch 3/100\n10/10 [==============================] - 42s 4s/step - loss: 0.8805 - f1_m: 0.4917 - val_loss: 0.8479 - val_f1_m: 0.4576\nEpoch 4/100\n10/10 [==============================] - 42s 4s/step - loss: 0.8608 - f1_m: 0.5217 - val_loss: 0.8100 - val_f1_m: 0.5209\nEpoch 5/100\n10/10 [==============================] - 43s 4s/step - loss: 0.8487 - f1_m: 0.5278 - val_loss: 0.7789 - val_f1_m: 0.5757\nEpoch 6/100\n10/10 [==============================] - 42s 4s/step - loss: 0.8012 - f1_m: 0.6024 - val_loss: 0.7725 - val_f1_m: 0.5664\nEpoch 7/100\n10/10 [==============================] - 43s 4s/step - loss: 0.8098 - f1_m: 0.5797 - val_loss: 0.7285 - val_f1_m: 0.6415\nEpoch 8/100\n10/10 [==============================] - 43s 4s/step - loss: 0.7749 - f1_m: 0.6164 - val_loss: 0.7093 - val_f1_m: 0.6574\nEpoch 9/100\n10/10 [==============================] - 43s 4s/step - loss: 0.7677 - f1_m: 0.6148 - val_loss: 0.6977 - val_f1_m: 0.6433\nEpoch 10/100\n10/10 [==============================] - 42s 4s/step - loss: 0.7317 - f1_m: 0.6404 - val_loss: 0.7300 - val_f1_m: 0.6397\nEpoch 11/100\n10/10 [==============================] - 42s 4s/step - loss: 0.7290 - f1_m: 0.6461 - val_loss: 0.6715 - val_f1_m: 0.6606\nEpoch 12/100\n10/10 [==============================] - 42s 4s/step - loss: 0.7260 - f1_m: 0.6479 - val_loss: 0.6693 - val_f1_m: 0.6754\nEpoch 13/100\n10/10 [==============================] - 42s 4s/step - loss: 0.7013 - f1_m: 0.6514 - val_loss: 0.6676 - val_f1_m: 0.6741\nEpoch 14/100\n10/10 [==============================] - 42s 4s/step - loss: 0.7042 - f1_m: 0.6650 - val_loss: 0.6728 - val_f1_m: 0.6671\nEpoch 15/100\n10/10 [==============================] - 43s 4s/step - loss: 0.6823 - f1_m: 0.6707 - val_loss: 0.6502 - val_f1_m: 0.6808\nEpoch 16/100\n10/10 [==============================] - 42s 4s/step - loss: 0.6778 - f1_m: 0.6829 - val_loss: 0.6510 - val_f1_m: 0.6871\nEpoch 17/100\n10/10 [==============================] - 42s 4s/step - loss: 0.6818 - f1_m: 0.6768 - val_loss: 0.6458 - val_f1_m: 0.7038\nEpoch 18/100\n10/10 [==============================] - 43s 4s/step - loss: 0.6603 - f1_m: 0.7068 - val_loss: 0.6257 - val_f1_m: 0.7182\nEpoch 19/100\n10/10 [==============================] - 42s 4s/step - loss: 0.6521 - f1_m: 0.6906 - val_loss: 0.6154 - val_f1_m: 0.7201\nEpoch 20/100\n10/10 [==============================] - 42s 4s/step - loss: 0.6358 - f1_m: 0.6970 - val_loss: 0.6206 - val_f1_m: 0.7105\nEpoch 21/100\n10/10 [==============================] - 42s 4s/step - loss: 0.6425 - f1_m: 0.7092 - val_loss: 0.6128 - val_f1_m: 0.7188\nEpoch 22/100\n10/10 [==============================] - 43s 4s/step - loss: 0.6122 - f1_m: 0.7103 - val_loss: 0.5970 - val_f1_m: 0.7247\nEpoch 23/100\n10/10 [==============================] - 41s 4s/step - loss: 0.6086 - f1_m: 0.7181 - val_loss: 0.6119 - val_f1_m: 0.7185\nEpoch 24/100\n10/10 [==============================] - 42s 4s/step - loss: 0.6249 - f1_m: 0.7110 - val_loss: 0.6125 - val_f1_m: 0.7166\nEpoch 25/100\n10/10 [==============================] - 42s 4s/step - loss: 0.5821 - f1_m: 0.7283 - val_loss: 0.5687 - val_f1_m: 0.7413\nEpoch 26/100\n10/10 [==============================] - 43s 4s/step - loss: 0.5836 - f1_m: 0.7317 - val_loss: 0.5899 - val_f1_m: 0.7202\nEpoch 27/100\n10/10 [==============================] - 42s 4s/step - loss: 0.5630 - f1_m: 0.7318 - val_loss: 0.5933 - val_f1_m: 0.7103\nEpoch 28/100\n10/10 [==============================] - 42s 4s/step - loss: 0.5751 - f1_m: 0.7298 - val_loss: 0.6103 - val_f1_m: 0.7082\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = model_bi.predict(xtest_pad)\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred , axis=1), labels = [0,1,2]))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:11:12.769716Z","iopub.execute_input":"2022-04-02T23:11:12.770225Z","iopub.status.idle":"2022-04-02T23:11:57.878168Z","shell.execute_reply.started":"2022-04-02T23:11:12.770159Z","shell.execute_reply":"2022-04-02T23:11:57.877417Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.77      0.82      0.79       626\n           1       0.81      0.64      0.71       373\n           2       0.45      0.54      0.49       170\n\n    accuracy                           0.72      1169\n   macro avg       0.67      0.67      0.66      1169\nweighted avg       0.73      0.72      0.72      1169\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Well bidrectional LSTM did not beat our baseline LSTM model so we are not going to consider this model**","metadata":{}},{"cell_type":"markdown","source":"# Model 11 GRU 2 layers","metadata":{}},{"cell_type":"code","source":"model_gru = Sequential()\nmodel_gru.add(Embedding(len(word_index)+1, 300, weights = [embedding_matrix], trainable=False))\n\nmodel_gru.add(SpatialDropout1D(0.3))\nmodel_gru.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences = True))\nmodel_gru.add(GRU(300, dropout = 0.3, recurrent_dropout = 0.3))\n\nmodel_gru.add(Dense(1024, activation = 'relu'))\nmodel_gru.add(Dropout(0.8))\n\nmodel_gru.add(Dense(1024, activation = 'relu'))\nmodel_gru.add(Dropout(0.8))\n\nmodel_gru.add(Dense(3, activation = 'softmax'))\nmodel_gru.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = f1_m_())\n\nearlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3, verbose = 0, mode= 'auto')\n\nmodel_gru.fit(xtrain_pad, y=y_train_enc, batch_size = 512, epochs = 100, verbose=1, validation_data = (xtest_pad, y_test_enc),callbacks = [earlystop])","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:11:57.879583Z","iopub.execute_input":"2022-04-02T23:11:57.879847Z","iopub.status.idle":"2022-04-02T23:29:24.667179Z","shell.execute_reply.started":"2022-04-02T23:11:57.879812Z","shell.execute_reply":"2022-04-02T23:29:24.666443Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Epoch 1/100\n10/10 [==============================] - 41s 4s/step - loss: 1.0026 - f1_m: 0.3537 - val_loss: 0.9422 - val_f1_m: 0.4370\nEpoch 2/100\n10/10 [==============================] - 35s 4s/step - loss: 0.9340 - f1_m: 0.4828 - val_loss: 0.8793 - val_f1_m: 0.5107\nEpoch 3/100\n10/10 [==============================] - 35s 4s/step - loss: 0.8953 - f1_m: 0.4970 - val_loss: 0.8315 - val_f1_m: 0.5169\nEpoch 4/100\n10/10 [==============================] - 35s 4s/step - loss: 0.8553 - f1_m: 0.5318 - val_loss: 0.8302 - val_f1_m: 0.5124\nEpoch 5/100\n10/10 [==============================] - 35s 4s/step - loss: 0.8424 - f1_m: 0.5528 - val_loss: 0.7881 - val_f1_m: 0.5818\nEpoch 6/100\n10/10 [==============================] - 35s 4s/step - loss: 0.8140 - f1_m: 0.5780 - val_loss: 0.7590 - val_f1_m: 0.6470\nEpoch 7/100\n10/10 [==============================] - 35s 4s/step - loss: 0.7984 - f1_m: 0.5958 - val_loss: 0.7515 - val_f1_m: 0.6309\nEpoch 8/100\n10/10 [==============================] - 35s 4s/step - loss: 0.7991 - f1_m: 0.6037 - val_loss: 0.7350 - val_f1_m: 0.6228\nEpoch 9/100\n10/10 [==============================] - 35s 3s/step - loss: 0.7839 - f1_m: 0.6137 - val_loss: 0.7299 - val_f1_m: 0.6240\nEpoch 10/100\n10/10 [==============================] - 35s 3s/step - loss: 0.7599 - f1_m: 0.6335 - val_loss: 0.6874 - val_f1_m: 0.6954\nEpoch 11/100\n10/10 [==============================] - 35s 3s/step - loss: 0.7496 - f1_m: 0.6378 - val_loss: 0.6895 - val_f1_m: 0.6555\nEpoch 12/100\n10/10 [==============================] - 36s 4s/step - loss: 0.7633 - f1_m: 0.6249 - val_loss: 0.7057 - val_f1_m: 0.6371\nEpoch 13/100\n10/10 [==============================] - 35s 4s/step - loss: 0.7450 - f1_m: 0.6384 - val_loss: 0.6668 - val_f1_m: 0.6859\nEpoch 14/100\n10/10 [==============================] - 36s 4s/step - loss: 0.7069 - f1_m: 0.6709 - val_loss: 0.6601 - val_f1_m: 0.6985\nEpoch 15/100\n10/10 [==============================] - 35s 4s/step - loss: 0.7201 - f1_m: 0.6629 - val_loss: 0.6493 - val_f1_m: 0.7016\nEpoch 16/100\n10/10 [==============================] - 36s 4s/step - loss: 0.7253 - f1_m: 0.6555 - val_loss: 0.6442 - val_f1_m: 0.7023\nEpoch 17/100\n10/10 [==============================] - 35s 4s/step - loss: 0.6841 - f1_m: 0.6603 - val_loss: 0.6669 - val_f1_m: 0.6694\nEpoch 18/100\n10/10 [==============================] - 35s 4s/step - loss: 0.6814 - f1_m: 0.6815 - val_loss: 0.6454 - val_f1_m: 0.6843\nEpoch 19/100\n10/10 [==============================] - 36s 4s/step - loss: 0.6858 - f1_m: 0.6822 - val_loss: 0.6238 - val_f1_m: 0.7176\nEpoch 20/100\n10/10 [==============================] - 35s 3s/step - loss: 0.6799 - f1_m: 0.6838 - val_loss: 0.6383 - val_f1_m: 0.7090\nEpoch 21/100\n10/10 [==============================] - 36s 4s/step - loss: 0.6547 - f1_m: 0.6960 - val_loss: 0.6040 - val_f1_m: 0.7030\nEpoch 22/100\n10/10 [==============================] - 35s 4s/step - loss: 0.6471 - f1_m: 0.6924 - val_loss: 0.5729 - val_f1_m: 0.7226\nEpoch 23/100\n10/10 [==============================] - 36s 4s/step - loss: 0.6287 - f1_m: 0.7172 - val_loss: 0.5794 - val_f1_m: 0.7317\nEpoch 24/100\n10/10 [==============================] - 36s 4s/step - loss: 0.6126 - f1_m: 0.7036 - val_loss: 0.5838 - val_f1_m: 0.7445\nEpoch 25/100\n10/10 [==============================] - 35s 4s/step - loss: 0.6075 - f1_m: 0.7041 - val_loss: 0.5674 - val_f1_m: 0.7360\nEpoch 26/100\n10/10 [==============================] - 36s 4s/step - loss: 0.5901 - f1_m: 0.7311 - val_loss: 0.5582 - val_f1_m: 0.7489\nEpoch 27/100\n10/10 [==============================] - 34s 3s/step - loss: 0.5793 - f1_m: 0.7425 - val_loss: 0.5602 - val_f1_m: 0.7384\nEpoch 28/100\n10/10 [==============================] - 35s 4s/step - loss: 0.5777 - f1_m: 0.7289 - val_loss: 0.5623 - val_f1_m: 0.7327\nEpoch 29/100\n10/10 [==============================] - 35s 3s/step - loss: 0.5639 - f1_m: 0.7433 - val_loss: 0.5618 - val_f1_m: 0.7487\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fa5ec2c5410>"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = model_gru.predict(xtest_pad)\n\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred , axis=1), labels = [0,1,2]))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:29:24.668495Z","iopub.execute_input":"2022-04-02T23:29:24.669225Z","iopub.status.idle":"2022-04-02T23:29:30.649454Z","shell.execute_reply.started":"2022-04-02T23:29:24.669186Z","shell.execute_reply":"2022-04-02T23:29:30.648701Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.77      0.88      0.82       626\n           1       0.77      0.80      0.78       373\n           2       0.67      0.27      0.38       170\n\n    accuracy                           0.76      1169\n   macro avg       0.73      0.65      0.66      1169\nweighted avg       0.75      0.76      0.74      1169\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Well not much difference between baseline single layer LSTM model to GRU model, but it looks our baseline model is still better.**\n\n**Lets add one more LSTM layer to baseline model and see what happens**","metadata":{}},{"cell_type":"markdown","source":"# Model 12 LSTM multiple layers","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3 ,  return_sequences = True))\nmodel.add(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics = [f1_m_()])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])\n\ny_pred = model.predict(xtest_pad)\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred, axis=1), labels=[0,1,2]))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:29:30.650628Z","iopub.execute_input":"2022-04-02T23:29:30.650899Z","iopub.status.idle":"2022-04-02T23:40:05.850793Z","shell.execute_reply.started":"2022-04-02T23:29:30.650864Z","shell.execute_reply":"2022-04-02T23:40:05.850102Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Epoch 1/100\n10/10 [==============================] - 50s 4s/step - loss: 1.0065 - f1_m: 0.3221 - val_loss: 0.9064 - val_f1_m: 0.5229\nEpoch 2/100\n10/10 [==============================] - 44s 4s/step - loss: 0.9024 - f1_m: 0.4942 - val_loss: 0.8612 - val_f1_m: 0.4992\nEpoch 3/100\n10/10 [==============================] - 44s 4s/step - loss: 0.8686 - f1_m: 0.4968 - val_loss: 0.8331 - val_f1_m: 0.5492\nEpoch 4/100\n10/10 [==============================] - 45s 4s/step - loss: 0.8563 - f1_m: 0.5242 - val_loss: 0.8032 - val_f1_m: 0.5638\nEpoch 5/100\n10/10 [==============================] - 43s 4s/step - loss: 0.8218 - f1_m: 0.5670 - val_loss: 0.7723 - val_f1_m: 0.5763\nEpoch 6/100\n10/10 [==============================] - 45s 4s/step - loss: 0.7867 - f1_m: 0.6002 - val_loss: 0.7206 - val_f1_m: 0.6562\nEpoch 7/100\n10/10 [==============================] - 44s 4s/step - loss: 0.7828 - f1_m: 0.6123 - val_loss: 0.6940 - val_f1_m: 0.6721\nEpoch 8/100\n10/10 [==============================] - 44s 4s/step - loss: 0.7572 - f1_m: 0.5934 - val_loss: 0.6945 - val_f1_m: 0.6554\nEpoch 9/100\n10/10 [==============================] - 45s 4s/step - loss: 0.7690 - f1_m: 0.6134 - val_loss: 0.7103 - val_f1_m: 0.6580\nEpoch 10/100\n10/10 [==============================] - 44s 4s/step - loss: 0.7247 - f1_m: 0.6470 - val_loss: 0.6591 - val_f1_m: 0.6931\nEpoch 11/100\n10/10 [==============================] - 44s 4s/step - loss: 0.7078 - f1_m: 0.6377 - val_loss: 0.6777 - val_f1_m: 0.6919\nEpoch 12/100\n10/10 [==============================] - 44s 4s/step - loss: 0.7198 - f1_m: 0.6491 - val_loss: 0.6723 - val_f1_m: 0.6618\nEpoch 13/100\n10/10 [==============================] - 44s 4s/step - loss: 0.7081 - f1_m: 0.6649 - val_loss: 0.6618 - val_f1_m: 0.6684\n              precision    recall  f1-score   support\n\n           0       0.72      0.89      0.79       626\n           1       0.78      0.51      0.62       373\n           2       0.47      0.42      0.45       170\n\n    accuracy                           0.70      1169\n   macro avg       0.66      0.61      0.62      1169\nweighted avg       0.70      0.70      0.69      1169\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Well the additional LSTM layer has imporoved the first 2 labels and did not imporove much for third label**","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\n#model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3 ,  return_sequences = True))\n#model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3 ,  return_sequences = True))\nmodel.add(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3))\n\nmodel.add(Dense(1024, activation='relu'))\n#model.add(Dropout(0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics = [f1_m_()])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])\n\ny_pred = model.predict(xtest_pad)\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred, axis=1), labels=[0,1,2]))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:40:05.852076Z","iopub.execute_input":"2022-04-02T23:40:05.852345Z","iopub.status.idle":"2022-04-02T23:44:34.507536Z","shell.execute_reply.started":"2022-04-02T23:40:05.852309Z","shell.execute_reply":"2022-04-02T23:44:34.506796Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch 1/100\n10/10 [==============================] - 25s 2s/step - loss: 0.9689 - f1_m: 0.4195 - val_loss: 0.9028 - val_f1_m: 0.4339\nEpoch 2/100\n10/10 [==============================] - 21s 2s/step - loss: 0.8798 - f1_m: 0.5013 - val_loss: 0.8053 - val_f1_m: 0.6134\nEpoch 3/100\n10/10 [==============================] - 21s 2s/step - loss: 0.8224 - f1_m: 0.5821 - val_loss: 0.7991 - val_f1_m: 0.5653\nEpoch 4/100\n10/10 [==============================] - 22s 2s/step - loss: 0.7800 - f1_m: 0.5996 - val_loss: 0.7562 - val_f1_m: 0.5994\nEpoch 5/100\n10/10 [==============================] - 21s 2s/step - loss: 0.7533 - f1_m: 0.6271 - val_loss: 0.6946 - val_f1_m: 0.6523\nEpoch 6/100\n10/10 [==============================] - 22s 2s/step - loss: 0.7288 - f1_m: 0.6378 - val_loss: 0.6959 - val_f1_m: 0.6434\nEpoch 7/100\n10/10 [==============================] - 21s 2s/step - loss: 0.7313 - f1_m: 0.6296 - val_loss: 0.6813 - val_f1_m: 0.6583\nEpoch 8/100\n10/10 [==============================] - 21s 2s/step - loss: 0.7072 - f1_m: 0.6654 - val_loss: 0.6906 - val_f1_m: 0.6454\nEpoch 9/100\n10/10 [==============================] - 22s 2s/step - loss: 0.6830 - f1_m: 0.6722 - val_loss: 0.6489 - val_f1_m: 0.6910\nEpoch 10/100\n10/10 [==============================] - 22s 2s/step - loss: 0.6800 - f1_m: 0.6715 - val_loss: 0.6710 - val_f1_m: 0.6785\nEpoch 11/100\n10/10 [==============================] - 21s 2s/step - loss: 0.6628 - f1_m: 0.6776 - val_loss: 0.6548 - val_f1_m: 0.6758\nEpoch 12/100\n10/10 [==============================] - 22s 2s/step - loss: 0.6663 - f1_m: 0.6750 - val_loss: 0.6726 - val_f1_m: 0.6821\n              precision    recall  f1-score   support\n\n           0       0.79      0.79      0.79       626\n           1       0.65      0.71      0.68       373\n           2       0.43      0.36      0.39       170\n\n    accuracy                           0.70      1169\n   macro avg       0.62      0.62      0.62      1169\nweighted avg       0.69      0.70      0.70      1169\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     trainable=False))\n#model.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, recurrent_dropout = 0.1))\n\nmodel.add(Dense(1024, activation='relu'))\n#model.add(Dropout(0.3))\nmodel.add(Dense(1024, activation='relu'))\n\nmodel.add(Dense(512, activation='relu'))\n#model.add(Dropout(0.3))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics = [f1_m_()])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])\n\ny_pred = model.predict(xtest_pad)\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred, axis=1), labels=[0,1,2]))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:44:34.508923Z","iopub.execute_input":"2022-04-02T23:44:34.50917Z","iopub.status.idle":"2022-04-02T23:47:46.03499Z","shell.execute_reply.started":"2022-04-02T23:44:34.509135Z","shell.execute_reply":"2022-04-02T23:47:46.034254Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Epoch 1/100\n10/10 [==============================] - 21s 2s/step - loss: 0.9545 - f1_m: 0.4164 - val_loss: 0.8713 - val_f1_m: 0.5208\nEpoch 2/100\n10/10 [==============================] - 17s 2s/step - loss: 0.8373 - f1_m: 0.5440 - val_loss: 0.7659 - val_f1_m: 0.6297\nEpoch 3/100\n10/10 [==============================] - 19s 2s/step - loss: 0.7452 - f1_m: 0.6289 - val_loss: 0.8002 - val_f1_m: 0.6507\nEpoch 4/100\n10/10 [==============================] - 18s 2s/step - loss: 0.6952 - f1_m: 0.6590 - val_loss: 0.7000 - val_f1_m: 0.6801\nEpoch 5/100\n10/10 [==============================] - 18s 2s/step - loss: 0.6336 - f1_m: 0.6931 - val_loss: 0.6736 - val_f1_m: 0.6942\nEpoch 6/100\n10/10 [==============================] - 19s 2s/step - loss: 0.5936 - f1_m: 0.7326 - val_loss: 0.7198 - val_f1_m: 0.6887\nEpoch 7/100\n10/10 [==============================] - 20s 2s/step - loss: 0.5574 - f1_m: 0.7449 - val_loss: 0.7074 - val_f1_m: 0.7196\nEpoch 8/100\n10/10 [==============================] - 19s 2s/step - loss: 0.4924 - f1_m: 0.7744 - val_loss: 0.7180 - val_f1_m: 0.6798\nEpoch 9/100\n10/10 [==============================] - 19s 2s/step - loss: 0.4595 - f1_m: 0.7982 - val_loss: 0.7091 - val_f1_m: 0.7175\nEpoch 10/100\n10/10 [==============================] - 19s 2s/step - loss: 0.4078 - f1_m: 0.8027 - val_loss: 0.7298 - val_f1_m: 0.7097\n              precision    recall  f1-score   support\n\n           0       0.79      0.75      0.77       626\n           1       0.62      0.85      0.72       373\n           2       0.56      0.21      0.30       170\n\n    accuracy                           0.70      1169\n   macro avg       0.66      0.60      0.60      1169\nweighted avg       0.70      0.70      0.68      1169\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     trainable=False))\n#model.add(SpatialDropout1D(0.3))\n#model.add(LSTM(300, recurrent_dropout = 0.1,return_sequences = True ))\nmodel.add(LSTM(300, recurrent_dropout = 0.1))\n\nmodel.add(Dense(2000, activation='relu'))\n\nmodel.add(Dense(2000, activation='relu'))\n#model.add(Dropout(0.3))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dense(750, activation='relu'))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics = [f1_m_()])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])\n\ny_pred = model.predict(xtest_pad)\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred, axis=1), labels=[0,1,2]))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:47:46.036446Z","iopub.execute_input":"2022-04-02T23:47:46.036696Z","iopub.status.idle":"2022-04-02T23:53:14.071242Z","shell.execute_reply.started":"2022-04-02T23:47:46.036663Z","shell.execute_reply":"2022-04-02T23:53:14.070426Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Epoch 1/100\n10/10 [==============================] - 24s 2s/step - loss: 1.0092 - f1_m: 0.2535 - val_loss: 0.9111 - val_f1_m: 0.5105\nEpoch 2/100\n10/10 [==============================] - 20s 2s/step - loss: 0.8909 - f1_m: 0.5046 - val_loss: 0.8892 - val_f1_m: 0.4454\nEpoch 3/100\n10/10 [==============================] - 20s 2s/step - loss: 0.8554 - f1_m: 0.5021 - val_loss: 0.9099 - val_f1_m: 0.5465\nEpoch 4/100\n10/10 [==============================] - 20s 2s/step - loss: 0.8458 - f1_m: 0.5260 - val_loss: 0.8429 - val_f1_m: 0.4604\nEpoch 5/100\n10/10 [==============================] - 20s 2s/step - loss: 0.7878 - f1_m: 0.5959 - val_loss: 0.7810 - val_f1_m: 0.5918\nEpoch 6/100\n10/10 [==============================] - 20s 2s/step - loss: 0.7143 - f1_m: 0.6572 - val_loss: 1.0242 - val_f1_m: 0.6692\nEpoch 7/100\n10/10 [==============================] - 19s 2s/step - loss: 0.7626 - f1_m: 0.5902 - val_loss: 0.8869 - val_f1_m: 0.5826\nEpoch 8/100\n10/10 [==============================] - 20s 2s/step - loss: 0.7278 - f1_m: 0.6123 - val_loss: 0.7970 - val_f1_m: 0.5939\nEpoch 9/100\n10/10 [==============================] - 19s 2s/step - loss: 0.6568 - f1_m: 0.6738 - val_loss: 0.7516 - val_f1_m: 0.6367\nEpoch 10/100\n10/10 [==============================] - 21s 2s/step - loss: 0.5945 - f1_m: 0.7076 - val_loss: 0.7568 - val_f1_m: 0.6662\nEpoch 11/100\n10/10 [==============================] - 21s 2s/step - loss: 0.5368 - f1_m: 0.7353 - val_loss: 0.7429 - val_f1_m: 0.6618\nEpoch 12/100\n10/10 [==============================] - 20s 2s/step - loss: 0.4910 - f1_m: 0.7854 - val_loss: 0.8251 - val_f1_m: 0.6695\nEpoch 13/100\n10/10 [==============================] - 20s 2s/step - loss: 0.4669 - f1_m: 0.7901 - val_loss: 0.8944 - val_f1_m: 0.6633\nEpoch 14/100\n10/10 [==============================] - 19s 2s/step - loss: 0.4840 - f1_m: 0.7916 - val_loss: 0.8659 - val_f1_m: 0.6758\nEpoch 15/100\n10/10 [==============================] - 20s 2s/step - loss: 0.4144 - f1_m: 0.8169 - val_loss: 0.7814 - val_f1_m: 0.7114\nEpoch 16/100\n10/10 [==============================] - 19s 2s/step - loss: 0.3331 - f1_m: 0.8392 - val_loss: 0.8004 - val_f1_m: 0.6959\n              precision    recall  f1-score   support\n\n           0       0.75      0.78      0.77       626\n           1       0.72      0.77      0.74       373\n           2       0.43      0.31      0.36       170\n\n    accuracy                           0.71      1169\n   macro avg       0.63      0.62      0.62      1169\nweighted avg       0.69      0.71      0.70      1169\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**This model works better for the label 2 class** ","metadata":{}},{"cell_type":"markdown","source":"# Classify text with BERT \n**this model needs more memory to run, so I cannot run it**","metadata":{}},{"cell_type":"code","source":"bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_base/2',\n    'electra_small':\n        'https://tfhub.dev/google/electra_small/2',\n    'electra_base':\n        'https://tfhub.dev/google/electra_base/2',\n    'experts_pubmed':\n        'https://tfhub.dev/google/experts/bert/pubmed/2',\n    'experts_wiki_books':\n        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'electra_small':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'electra_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_pubmed':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_wiki_books':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:53:14.07269Z","iopub.execute_input":"2022-04-02T23:53:14.072949Z","iopub.status.idle":"2022-04-02T23:53:14.090763Z","shell.execute_reply.started":"2022-04-02T23:53:14.072914Z","shell.execute_reply":"2022-04-02T23:53:14.089622Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\nPreprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:53:14.091961Z","iopub.execute_input":"2022-04-02T23:53:14.092624Z","iopub.status.idle":"2022-04-02T23:53:17.412644Z","shell.execute_reply.started":"2022-04-02T23:53:14.092583Z","shell.execute_reply":"2022-04-02T23:53:17.411916Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"bert_model = hub.KerasLayer(tfhub_handle_encoder)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:53:17.41438Z","iopub.execute_input":"2022-04-02T23:53:17.415132Z","iopub.status.idle":"2022-04-02T23:53:23.761731Z","shell.execute_reply.started":"2022-04-02T23:53:17.415091Z","shell.execute_reply":"2022-04-02T23:53:23.761013Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def build_classifier_model():\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(3, activation='softmax', name='classifier')(net)\n    return tf.keras.Model(text_input, net)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:53:23.763139Z","iopub.execute_input":"2022-04-02T23:53:23.763396Z","iopub.status.idle":"2022-04-02T23:53:23.770322Z","shell.execute_reply.started":"2022-04-02T23:53:23.763361Z","shell.execute_reply":"2022-04-02T23:53:23.769216Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(df.Sentence, df.Sentiment, test_size = 0.2, random_state = 42)\ny_train_enc = np_utils.to_categorical(y_train, 3)\ny_test_enc = np_utils.to_categorical(y_test, 3)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:53:23.771793Z","iopub.execute_input":"2022-04-02T23:53:23.772044Z","iopub.status.idle":"2022-04-02T23:53:23.782648Z","shell.execute_reply.started":"2022-04-02T23:53:23.772011Z","shell.execute_reply":"2022-04-02T23:53:23.781945Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"classifier_model = build_classifier_model()\nclassifier_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics = [f1_m_()])\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\nclassifier_model.fit(X_train, y=y_train_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data = (X_test, y_test_enc), callbacks=[earlystop])\n\ny_pred = classifier_model.predict(X_test)\nprint(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred, axis=1), labels=[0,1,2]))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:53:23.783988Z","iopub.execute_input":"2022-04-02T23:53:23.78425Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}